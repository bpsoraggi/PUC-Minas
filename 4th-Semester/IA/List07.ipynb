{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JA1xfNLHiTY8",
        "b-1KNLWqh7Vw",
        "5mGcTWMRWppq",
        "3pSCMI5AZyFD",
        "qWJHEkxYZ3eY",
        "qOTMYFQFbwFZ",
        "Nbqqw5PacRWn",
        "W6ojpdr2fTPa",
        "yw-9yA6FgYTx",
        "v9DHW9Exhh8e",
        "EUtvEM2Cpm_R"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Questão 01"
      ],
      "metadata": {
        "id": "JA1xfNLHiTY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, num_inputs, learning_rate=0.01, epochs=1000):\n",
        "        self.weights = np.random.rand(num_inputs)\n",
        "        self.bias = np.random.rand()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        summation = np.dot(inputs, self.weights) + self.bias\n",
        "        return self.activation_function(summation)\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        for epoch in range(self.epochs):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = label - prediction\n",
        "                self.weights += self.learning_rate * error * inputs\n",
        "                self.bias += self.learning_rate * error\n",
        "\n",
        "def generate_inputs_labels(num_inputs, logic_gate):\n",
        "    inputs = np.random.randint(2, size=(2**num_inputs, num_inputs))\n",
        "    if logic_gate == \"and\":\n",
        "        labels = np.all(inputs, axis=1)\n",
        "    elif logic_gate == \"or\":\n",
        "        labels = np.any(inputs, axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"A porta lógica deve ser 'and' ou 'or'\")\n",
        "    return inputs, labels.astype(int)\n",
        "\n",
        "def test_perceptron(num_inputs, logic_gate):\n",
        "    inputs, labels = generate_inputs_labels(num_inputs, logic_gate)\n",
        "    perceptron = Perceptron(num_inputs)\n",
        "\n",
        "    print(f\"\\nTreinando o Perceptron para o problema de {logic_gate.upper()} com {num_inputs} entradas:\")\n",
        "    print(\"Pesos iniciais:\", perceptron.weights)\n",
        "    print(\"Bias inicial:\", perceptron.bias)\n",
        "\n",
        "    perceptron.train(inputs, labels)\n",
        "\n",
        "    print(\"\\nPesos treinados:\", perceptron.weights)\n",
        "    print(\"Bias treinado:\", perceptron.bias)\n",
        "\n",
        "    print(\"\\nTestando o Perceptron treinado:\")\n",
        "    for i, input_set in enumerate(inputs):\n",
        "        prediction = perceptron.predict(input_set)\n",
        "        print(f\"Entrada: {input_set}, Previsto: {prediction}, Real: {labels[i]}\")\n",
        "\n",
        "num_inputs = int(input(\"Digite o número de entradas para o Perceptron (por exemplo, 2, 3, ...): \"))\n",
        "logic_gate = input(\"Digite a porta lógica desejada ('and' ou 'or'): \").lower()\n",
        "\n",
        "test_perceptron(num_inputs, logic_gate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbLQFLwuiWVi",
        "outputId": "a1cb6927-766c-4426-f9b7-3c62c43888e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite o número de entradas para o Perceptron (por exemplo, 2, 3, ...): 2\n",
            "Digite a porta lógica desejada ('and' ou 'or'): and\n",
            "\n",
            "Treinando o Perceptron para o problema de AND com 2 entradas:\n",
            "Pesos iniciais: [0.05961426 0.87950533]\n",
            "Bias inicial: 0.11748656128237867\n",
            "\n",
            "Pesos treinados: [0.00961426 0.87950533]\n",
            "Bias treinado: -0.01251343871762132\n",
            "\n",
            "Testando o Perceptron treinado:\n",
            "Entrada: [0 0], Previsto: 0, Real: 0\n",
            "Entrada: [0 0], Previsto: 0, Real: 0\n",
            "Entrada: [1 1], Previsto: 1, Real: 1\n",
            "Entrada: [1 0], Previsto: 0, Real: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, pode-se opbservar que, pesar do treinamento, o Perceptron não será capaz de resolver corretamente o problema XOR. Isso ocorre porque **o XOR não é linearmente separável**, e **o Perceptron é um classificador linear**, incapaz de encontrar uma linha (ou hiperplano) que separe as classes 0 e 1 de forma adequada."
      ],
      "metadata": {
        "id": "qLfIMG-xpLGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_xor():\n",
        "    xor_inputs_2 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    xor_labels_2 = np.array([0, 1, 1, 0])\n",
        "\n",
        "    perceptron_xor = Perceptron(2)\n",
        "\n",
        "    print(\"\\nTreinando o Perceptron para o problema XOR com 2 entradas:\")\n",
        "    print(\"Pesos iniciais:\", perceptron_xor.weights)\n",
        "    print(\"Bias inicial:\", perceptron_xor.bias)\n",
        "\n",
        "    perceptron_xor.train(xor_inputs_2, xor_labels_2)\n",
        "\n",
        "    print(\"\\nPesos treinados:\", perceptron_xor.weights)\n",
        "    print(\"Bias treinado:\", perceptron_xor.bias)\n",
        "\n",
        "    print(\"\\nTestando o Perceptron treinado para XOR:\")\n",
        "    for i, input_set in enumerate(xor_inputs_2):\n",
        "        prediction = perceptron_xor.predict(input_set)\n",
        "        print(f\"Entrada: {input_set}, Previsto: {prediction}, Real: {xor_labels_2[i]}\")\n",
        "\n",
        "# Testar o Perceptron para o problema XOR\n",
        "test_xor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuhjpm2kpIY5",
        "outputId": "03f974ae-ba89-436e-af1f-a44c766e94e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando o Perceptron para o problema XOR com 2 entradas:\n",
            "Pesos iniciais: [0.99039722 0.66627817]\n",
            "Bias inicial: 0.2618841091451878\n",
            "\n",
            "Pesos treinados: [-0.00960278  0.00627817]\n",
            "Bias treinado: 0.0018841091451876936\n",
            "\n",
            "Testando o Perceptron treinado para XOR:\n",
            "Entrada: [0 0], Previsto: 1, Real: 0\n",
            "Entrada: [0 1], Previsto: 1, Real: 1\n",
            "Entrada: [1 0], Previsto: 0, Real: 1\n",
            "Entrada: [1 1], Previsto: 0, Real: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questões 02 - 04"
      ],
      "metadata": {
        "id": "b-1KNLWqh7Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Letra d\n",
        "3.   Letra c\n",
        "4.   Letra c"
      ],
      "metadata": {
        "id": "c_zq11qHh_Et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código original"
      ],
      "metadata": {
        "id": "5mGcTWMRWppq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import pickle\n",
        "\n",
        "local_file_path = 'cancer.pkl'\n",
        "\n",
        "with open(local_file_path, 'rb') as f:\n",
        "    X_treino, X_teste, y_treino, y_teste = pickle.load(f)\n",
        "\n",
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)\n",
        "\n",
        "rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00000000000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = 9)\n",
        "modelo.fit(X_treino, y_treino)\n",
        "\n",
        "previsoes = modelo.predict(X_teste)\n",
        "\n",
        "previsoes\n",
        "y_teste\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_teste,previsoes)\n",
        "\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "confusion_matrix(y_teste, previsoes)\n",
        "\n",
        "cm = ConfusionMatrix(modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)\n",
        "\n",
        "print(classification_report(y_teste, previsoes))"
      ],
      "metadata": {
        "id": "YX046JUzWEoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão 05"
      ],
      "metadata": {
        "id": "3pSCMI5AZyFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1"
      ],
      "metadata": {
        "id": "qWJHEkxYZ3eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "# Carregar o arquivo CSV\n",
        "df = pd.read_csv('/content/breast-cancer.csv')\n",
        "\n",
        "# Separar os dados em features (X) e rótulos (y)\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Dividir os dados em conjuntos de treino e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convertendo colunas categóricas para numéricas usando LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Ajustar o LabelEncoder apenas nos dados de treino\n",
        "y_treino_encoded = le.fit_transform(y_treino)\n",
        "\n",
        "# Aplicar a transformação nos dados de teste\n",
        "y_teste_encoded = le.transform(y_teste)\n",
        "\n",
        "# Lidar com variáveis categóricas usando OneHotEncoder\n",
        "categorical_cols = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\n",
        "\n",
        "# Juntar os dados de treino e teste antes de aplicar o OneHotEncoder\n",
        "X_combined = pd.concat([X_treino, X_teste], axis=0)\n",
        "X_combined_encoded = pd.get_dummies(X_combined, columns=categorical_cols)\n",
        "\n",
        "# Separar os dados de volta em treino e teste\n",
        "X_treino_encoded = X_combined_encoded.iloc[:len(X_treino)]\n",
        "X_teste_encoded = X_combined_encoded.iloc[len(X_treino):]\n",
        "\n",
        "# Normalização\n",
        "scaler = MinMaxScaler()\n",
        "X_treino_normalized = scaler.fit_transform(X_treino_encoded)\n",
        "X_teste_normalized = scaler.transform(X_teste_encoded)\n",
        "\n",
        "# Balanceamento\n",
        "smote = SMOTE()\n",
        "X_treino_balanced, y_treino_balanced = smote.fit_resample(X_treino_normalized, y_treino_encoded)\n",
        "\n",
        "# Restante do seu código (modelo, treinamento, previsões, métricas) permanece o mesmo\n",
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "previsoes = modelo.predict(X_teste_normalized)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(y_teste_encoded, previsoes))\n",
        "\n",
        "# Confusion Matrix\n",
        "confusion_matrix_plot = ConfusionMatrix(modelo)\n",
        "confusion_matrix_plot.fit(X_treino_balanced, y_treino_balanced)\n",
        "confusion_matrix_plot.score(X_teste_normalized, y_teste_encoded)\n",
        "confusion_matrix_plot.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_teste_encoded, previsoes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YSTU7gkeXH35",
        "outputId": "3263d6c7-9f0c-41fa-e086-54dfb2d7bb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.69037194\n",
            "Iteration 2, loss = 0.67828897\n",
            "Iteration 3, loss = 0.66885711\n",
            "Iteration 4, loss = 0.65966414\n",
            "Iteration 5, loss = 0.65078216\n",
            "Iteration 6, loss = 0.64282176\n",
            "Iteration 7, loss = 0.63535246\n",
            "Iteration 8, loss = 0.62820975\n",
            "Iteration 9, loss = 0.62183970\n",
            "Iteration 10, loss = 0.61528908\n",
            "Iteration 11, loss = 0.60963290\n",
            "Iteration 12, loss = 0.60391435\n",
            "Iteration 13, loss = 0.59915665\n",
            "Iteration 14, loss = 0.59392567\n",
            "Iteration 15, loss = 0.58938181\n",
            "Iteration 16, loss = 0.58451881\n",
            "Iteration 17, loss = 0.58065273\n",
            "Iteration 18, loss = 0.57655800\n",
            "Iteration 19, loss = 0.57261148\n",
            "Iteration 20, loss = 0.56867253\n",
            "Iteration 21, loss = 0.56515331\n",
            "Iteration 22, loss = 0.56154365\n",
            "Iteration 23, loss = 0.55834388\n",
            "Iteration 24, loss = 0.55479548\n",
            "Iteration 25, loss = 0.55158476\n",
            "Iteration 26, loss = 0.54845738\n",
            "Iteration 27, loss = 0.54549209\n",
            "Iteration 28, loss = 0.54236115\n",
            "Iteration 29, loss = 0.53930850\n",
            "Iteration 30, loss = 0.53635489\n",
            "Iteration 31, loss = 0.53325701\n",
            "Iteration 32, loss = 0.53003929\n",
            "Iteration 33, loss = 0.52712588\n",
            "Iteration 34, loss = 0.52420403\n",
            "Iteration 35, loss = 0.52118640\n",
            "Iteration 36, loss = 0.51831262\n",
            "Iteration 37, loss = 0.51540804\n",
            "Iteration 38, loss = 0.51238992\n",
            "Iteration 39, loss = 0.50928851\n",
            "Iteration 40, loss = 0.50618298\n",
            "Iteration 41, loss = 0.50331246\n",
            "Iteration 42, loss = 0.50035702\n",
            "Iteration 43, loss = 0.49725852\n",
            "Iteration 44, loss = 0.49448499\n",
            "Iteration 45, loss = 0.49138919\n",
            "Iteration 46, loss = 0.48850688\n",
            "Iteration 47, loss = 0.48570795\n",
            "Iteration 48, loss = 0.48267390\n",
            "Iteration 49, loss = 0.47953854\n",
            "Iteration 50, loss = 0.47664083\n",
            "Iteration 51, loss = 0.47387830\n",
            "Iteration 52, loss = 0.47061834\n",
            "Iteration 53, loss = 0.46760549\n",
            "Iteration 54, loss = 0.46468422\n",
            "Iteration 55, loss = 0.46159593\n",
            "Iteration 56, loss = 0.45846128\n",
            "Iteration 57, loss = 0.45545861\n",
            "Iteration 58, loss = 0.45259467\n",
            "Iteration 59, loss = 0.44949070\n",
            "Iteration 60, loss = 0.44622043\n",
            "Iteration 61, loss = 0.44317910\n",
            "Iteration 62, loss = 0.44009539\n",
            "Iteration 63, loss = 0.43719829\n",
            "Iteration 64, loss = 0.43409634\n",
            "Iteration 65, loss = 0.43104958\n",
            "Iteration 66, loss = 0.42795214\n",
            "Iteration 67, loss = 0.42493230\n",
            "Iteration 68, loss = 0.42213671\n",
            "Iteration 69, loss = 0.41905212\n",
            "Iteration 70, loss = 0.41610084\n",
            "Iteration 71, loss = 0.41313780\n",
            "Iteration 72, loss = 0.41037786\n",
            "Iteration 73, loss = 0.40714136\n",
            "Iteration 74, loss = 0.40419846\n",
            "Iteration 75, loss = 0.40126205\n",
            "Iteration 76, loss = 0.39826933\n",
            "Iteration 77, loss = 0.39554662\n",
            "Iteration 78, loss = 0.39256273\n",
            "Iteration 79, loss = 0.38965179\n",
            "Iteration 80, loss = 0.38684109\n",
            "Iteration 81, loss = 0.38378192\n",
            "Iteration 82, loss = 0.38100607\n",
            "Iteration 83, loss = 0.37830327\n",
            "Iteration 84, loss = 0.37592041\n",
            "Iteration 85, loss = 0.37305221\n",
            "Iteration 86, loss = 0.37011968\n",
            "Iteration 87, loss = 0.36727888\n",
            "Iteration 88, loss = 0.36474657\n",
            "Iteration 89, loss = 0.36222018\n",
            "Iteration 90, loss = 0.35928903\n",
            "Iteration 91, loss = 0.35680070\n",
            "Iteration 92, loss = 0.35422415\n",
            "Iteration 93, loss = 0.35155439\n",
            "Iteration 94, loss = 0.34893979\n",
            "Iteration 95, loss = 0.34637192\n",
            "Iteration 96, loss = 0.34369850\n",
            "Iteration 97, loss = 0.34136397\n",
            "Iteration 98, loss = 0.33852522\n",
            "Iteration 99, loss = 0.33616352\n",
            "Iteration 100, loss = 0.33350274\n",
            "Iteration 101, loss = 0.33107710\n",
            "Iteration 102, loss = 0.32862095\n",
            "Iteration 103, loss = 0.32660447\n",
            "Iteration 104, loss = 0.32415043\n",
            "Iteration 105, loss = 0.32156878\n",
            "Iteration 106, loss = 0.31932250\n",
            "Iteration 107, loss = 0.31681709\n",
            "Iteration 108, loss = 0.31449980\n",
            "Iteration 109, loss = 0.31222094\n",
            "Iteration 110, loss = 0.30996555\n",
            "Iteration 111, loss = 0.30772128\n",
            "Iteration 112, loss = 0.30533171\n",
            "Iteration 113, loss = 0.30300415\n",
            "Iteration 114, loss = 0.30100666\n",
            "Iteration 115, loss = 0.29875546\n",
            "Iteration 116, loss = 0.29648472\n",
            "Iteration 117, loss = 0.29439995\n",
            "Iteration 118, loss = 0.29207517\n",
            "Iteration 119, loss = 0.29014767\n",
            "Iteration 120, loss = 0.28786843\n",
            "Iteration 121, loss = 0.28572946\n",
            "Iteration 122, loss = 0.28394202\n",
            "Iteration 123, loss = 0.28156612\n",
            "Iteration 124, loss = 0.27994823\n",
            "Iteration 125, loss = 0.27766523\n",
            "Iteration 126, loss = 0.27548748\n",
            "Iteration 127, loss = 0.27341942\n",
            "Iteration 128, loss = 0.27158427\n",
            "Iteration 129, loss = 0.26954965\n",
            "Iteration 130, loss = 0.26764576\n",
            "Iteration 131, loss = 0.26572336\n",
            "Iteration 132, loss = 0.26374073\n",
            "Iteration 133, loss = 0.26181588\n",
            "Iteration 134, loss = 0.25985275\n",
            "Iteration 135, loss = 0.25795818\n",
            "Iteration 136, loss = 0.25611893\n",
            "Iteration 137, loss = 0.25445997\n",
            "Iteration 138, loss = 0.25249937\n",
            "Iteration 139, loss = 0.25074319\n",
            "Iteration 140, loss = 0.24887191\n",
            "Iteration 141, loss = 0.24721137\n",
            "Iteration 142, loss = 0.24539659\n",
            "Iteration 143, loss = 0.24358962\n",
            "Iteration 144, loss = 0.24193077\n",
            "Iteration 145, loss = 0.24013032\n",
            "Iteration 146, loss = 0.23857544\n",
            "Iteration 147, loss = 0.23677580\n",
            "Iteration 148, loss = 0.23534781\n",
            "Iteration 149, loss = 0.23333822\n",
            "Iteration 150, loss = 0.23194676\n",
            "Iteration 151, loss = 0.23043095\n",
            "Iteration 152, loss = 0.22865667\n",
            "Iteration 153, loss = 0.22693837\n",
            "Iteration 154, loss = 0.22543380\n",
            "Iteration 155, loss = 0.22377873\n",
            "Iteration 156, loss = 0.22231948\n",
            "Iteration 157, loss = 0.22079129\n",
            "Iteration 158, loss = 0.21922929\n",
            "Iteration 159, loss = 0.21761642\n",
            "Iteration 160, loss = 0.21607574\n",
            "Iteration 161, loss = 0.21464236\n",
            "Iteration 162, loss = 0.21309039\n",
            "Iteration 163, loss = 0.21200926\n",
            "Iteration 164, loss = 0.21008602\n",
            "Iteration 165, loss = 0.20877946\n",
            "Iteration 166, loss = 0.20722094\n",
            "Iteration 167, loss = 0.20577513\n",
            "Iteration 168, loss = 0.20444771\n",
            "Iteration 169, loss = 0.20298862\n",
            "Iteration 170, loss = 0.20157109\n",
            "Iteration 171, loss = 0.20032553\n",
            "Iteration 172, loss = 0.19879309\n",
            "Iteration 173, loss = 0.19737015\n",
            "Iteration 174, loss = 0.19603854\n",
            "Iteration 175, loss = 0.19467544\n",
            "Iteration 176, loss = 0.19359249\n",
            "Iteration 177, loss = 0.19233767\n",
            "Iteration 178, loss = 0.19071594\n",
            "Iteration 179, loss = 0.18954628\n",
            "Iteration 180, loss = 0.18830721\n",
            "Iteration 181, loss = 0.18685382\n",
            "Iteration 182, loss = 0.18630984\n",
            "Iteration 183, loss = 0.18442342\n",
            "Iteration 184, loss = 0.18307974\n",
            "Iteration 185, loss = 0.18212933\n",
            "Iteration 186, loss = 0.18102612\n",
            "Iteration 187, loss = 0.17978089\n",
            "Iteration 188, loss = 0.17844968\n",
            "Iteration 189, loss = 0.17726357\n",
            "Iteration 190, loss = 0.17627961\n",
            "Iteration 191, loss = 0.17513543\n",
            "Iteration 192, loss = 0.17393387\n",
            "Iteration 193, loss = 0.17278192\n",
            "Iteration 194, loss = 0.17172822\n",
            "Iteration 195, loss = 0.17052934\n",
            "Iteration 196, loss = 0.16947434\n",
            "Iteration 197, loss = 0.16835810\n",
            "Iteration 198, loss = 0.16726629\n",
            "Iteration 199, loss = 0.16627725\n",
            "Iteration 200, loss = 0.16523043\n",
            "Iteration 201, loss = 0.16419994\n",
            "Iteration 202, loss = 0.16303779\n",
            "Iteration 203, loss = 0.16235793\n",
            "Iteration 204, loss = 0.16101622\n",
            "Iteration 205, loss = 0.15986541\n",
            "Iteration 206, loss = 0.15897680\n",
            "Iteration 207, loss = 0.15813840\n",
            "Iteration 208, loss = 0.15741944\n",
            "Iteration 209, loss = 0.15590499\n",
            "Iteration 210, loss = 0.15495047\n",
            "Iteration 211, loss = 0.15407958\n",
            "Iteration 212, loss = 0.15317234\n",
            "Iteration 213, loss = 0.15224921\n",
            "Iteration 214, loss = 0.15122245\n",
            "Iteration 215, loss = 0.15023775\n",
            "Iteration 216, loss = 0.14955437\n",
            "Iteration 217, loss = 0.14832414\n",
            "Iteration 218, loss = 0.14760527\n",
            "Iteration 219, loss = 0.14670825\n",
            "Iteration 220, loss = 0.14589545\n",
            "Iteration 221, loss = 0.14482062\n",
            "Iteration 222, loss = 0.14394946\n",
            "Iteration 223, loss = 0.14315097\n",
            "Iteration 224, loss = 0.14242605\n",
            "Iteration 225, loss = 0.14142310\n",
            "Iteration 226, loss = 0.14072679\n",
            "Iteration 227, loss = 0.14028860\n",
            "Iteration 228, loss = 0.13950973\n",
            "Iteration 229, loss = 0.13835524\n",
            "Iteration 230, loss = 0.13754697\n",
            "Iteration 231, loss = 0.13666766\n",
            "Iteration 232, loss = 0.13585099\n",
            "Iteration 233, loss = 0.13516101\n",
            "Iteration 234, loss = 0.13434967\n",
            "Iteration 235, loss = 0.13351608\n",
            "Iteration 236, loss = 0.13285520\n",
            "Iteration 237, loss = 0.13207936\n",
            "Iteration 238, loss = 0.13142147\n",
            "Iteration 239, loss = 0.13055308\n",
            "Iteration 240, loss = 0.12980810\n",
            "Iteration 241, loss = 0.12912429\n",
            "Iteration 242, loss = 0.12843849\n",
            "Iteration 243, loss = 0.12770788\n",
            "Iteration 244, loss = 0.12694202\n",
            "Iteration 245, loss = 0.12633050\n",
            "Iteration 246, loss = 0.12555081\n",
            "Iteration 247, loss = 0.12500574\n",
            "Iteration 248, loss = 0.12424871\n",
            "Iteration 249, loss = 0.12362167\n",
            "Iteration 250, loss = 0.12306805\n",
            "Iteration 251, loss = 0.12226737\n",
            "Iteration 252, loss = 0.12157913\n",
            "Iteration 253, loss = 0.12108474\n",
            "Iteration 254, loss = 0.12047797\n",
            "Iteration 255, loss = 0.11980562\n",
            "Iteration 256, loss = 0.11907039\n",
            "Iteration 257, loss = 0.11852717\n",
            "Iteration 258, loss = 0.11791107\n",
            "Iteration 259, loss = 0.11724388\n",
            "Iteration 260, loss = 0.11685595\n",
            "Iteration 261, loss = 0.11615308\n",
            "Iteration 262, loss = 0.11543168\n",
            "Iteration 263, loss = 0.11479319\n",
            "Iteration 264, loss = 0.11431951\n",
            "Iteration 265, loss = 0.11377660\n",
            "Iteration 266, loss = 0.11319860\n",
            "Iteration 267, loss = 0.11263281\n",
            "Iteration 268, loss = 0.11205794\n",
            "Iteration 269, loss = 0.11141852\n",
            "Iteration 270, loss = 0.11084094\n",
            "Iteration 271, loss = 0.11036590\n",
            "Iteration 272, loss = 0.10977894\n",
            "Iteration 273, loss = 0.10912202\n",
            "Iteration 274, loss = 0.10867359\n",
            "Iteration 275, loss = 0.10824819\n",
            "Iteration 276, loss = 0.10768221\n",
            "Iteration 277, loss = 0.10695239\n",
            "Iteration 278, loss = 0.10640843\n",
            "Iteration 279, loss = 0.10643609\n",
            "Iteration 280, loss = 0.10567826\n",
            "Iteration 281, loss = 0.10497313\n",
            "Iteration 282, loss = 0.10443634\n",
            "Iteration 283, loss = 0.10406994\n",
            "Iteration 284, loss = 0.10355334\n",
            "Iteration 285, loss = 0.10290727\n",
            "Iteration 286, loss = 0.10242202\n",
            "Iteration 287, loss = 0.10201633\n",
            "Iteration 288, loss = 0.10146130\n",
            "Iteration 289, loss = 0.10120603\n",
            "Iteration 290, loss = 0.10055362\n",
            "Iteration 291, loss = 0.09986381\n",
            "Iteration 292, loss = 0.09979137\n",
            "Iteration 293, loss = 0.09908423\n",
            "Iteration 294, loss = 0.09842343\n",
            "Iteration 295, loss = 0.09795093\n",
            "Iteration 296, loss = 0.09750906\n",
            "Iteration 297, loss = 0.09708762\n",
            "Iteration 298, loss = 0.09670746\n",
            "Iteration 299, loss = 0.09614872\n",
            "Iteration 300, loss = 0.09583049\n",
            "Iteration 301, loss = 0.09525658\n",
            "Iteration 302, loss = 0.09483494\n",
            "Iteration 303, loss = 0.09444958\n",
            "Iteration 304, loss = 0.09405833\n",
            "Iteration 305, loss = 0.09356768\n",
            "Iteration 306, loss = 0.09315121\n",
            "Iteration 307, loss = 0.09286845\n",
            "Iteration 308, loss = 0.09227944\n",
            "Iteration 309, loss = 0.09185113\n",
            "Iteration 310, loss = 0.09149153\n",
            "Iteration 311, loss = 0.09111573\n",
            "Iteration 312, loss = 0.09060755\n",
            "Iteration 313, loss = 0.09021879\n",
            "Iteration 314, loss = 0.09003387\n",
            "Iteration 315, loss = 0.08943032\n",
            "Iteration 316, loss = 0.08898174\n",
            "Iteration 317, loss = 0.08876487\n",
            "Iteration 318, loss = 0.08849937\n",
            "Iteration 319, loss = 0.08816578\n",
            "Iteration 320, loss = 0.08753968\n",
            "Iteration 321, loss = 0.08723482\n",
            "Iteration 322, loss = 0.08702430\n",
            "Iteration 323, loss = 0.08661329\n",
            "Iteration 324, loss = 0.08620842\n",
            "Iteration 325, loss = 0.08578319\n",
            "Iteration 326, loss = 0.08541280\n",
            "Iteration 327, loss = 0.08497160\n",
            "Iteration 328, loss = 0.08458283\n",
            "Iteration 329, loss = 0.08424433\n",
            "Iteration 330, loss = 0.08406107\n",
            "Iteration 331, loss = 0.08363345\n",
            "Iteration 332, loss = 0.08322109\n",
            "Iteration 333, loss = 0.08284218\n",
            "Iteration 334, loss = 0.08257885\n",
            "Iteration 335, loss = 0.08216531\n",
            "Iteration 336, loss = 0.08182390\n",
            "Iteration 337, loss = 0.08145407\n",
            "Iteration 338, loss = 0.08113237\n",
            "Iteration 339, loss = 0.08083900\n",
            "Iteration 340, loss = 0.08052567\n",
            "Iteration 341, loss = 0.08010827\n",
            "Iteration 342, loss = 0.07986512\n",
            "Iteration 343, loss = 0.07969740\n",
            "Iteration 344, loss = 0.07917096\n",
            "Iteration 345, loss = 0.07891716\n",
            "Iteration 346, loss = 0.07861074\n",
            "Iteration 347, loss = 0.07828993\n",
            "Iteration 348, loss = 0.07791567\n",
            "Iteration 349, loss = 0.07772830\n",
            "Iteration 350, loss = 0.07743326\n",
            "Iteration 351, loss = 0.07704285\n",
            "Iteration 352, loss = 0.07677829\n",
            "Iteration 353, loss = 0.07651131\n",
            "Iteration 354, loss = 0.07612414\n",
            "Iteration 355, loss = 0.07583749\n",
            "Iteration 356, loss = 0.07559878\n",
            "Iteration 357, loss = 0.07529275\n",
            "Iteration 358, loss = 0.07495192\n",
            "Iteration 359, loss = 0.07473654\n",
            "Iteration 360, loss = 0.07437353\n",
            "Iteration 361, loss = 0.07404912\n",
            "Iteration 362, loss = 0.07380462\n",
            "Iteration 363, loss = 0.07349455\n",
            "Iteration 364, loss = 0.07321339\n",
            "Iteration 365, loss = 0.07304395\n",
            "Iteration 366, loss = 0.07274415\n",
            "Iteration 367, loss = 0.07238106\n",
            "Iteration 368, loss = 0.07220159\n",
            "Iteration 369, loss = 0.07190094\n",
            "Iteration 370, loss = 0.07153786\n",
            "Iteration 371, loss = 0.07138662\n",
            "Iteration 372, loss = 0.07112210\n",
            "Iteration 373, loss = 0.07092074\n",
            "Iteration 374, loss = 0.07060864\n",
            "Iteration 375, loss = 0.07032548\n",
            "Iteration 376, loss = 0.07019482\n",
            "Iteration 377, loss = 0.06989291\n",
            "Iteration 378, loss = 0.06958765\n",
            "Iteration 379, loss = 0.06925647\n",
            "Iteration 380, loss = 0.06914293\n",
            "Iteration 381, loss = 0.06874199\n",
            "Iteration 382, loss = 0.06854252\n",
            "Iteration 383, loss = 0.06833435\n",
            "Iteration 384, loss = 0.06804852\n",
            "Iteration 385, loss = 0.06794864\n",
            "Iteration 386, loss = 0.06758630\n",
            "Iteration 387, loss = 0.06732002\n",
            "Iteration 388, loss = 0.06706221\n",
            "Iteration 389, loss = 0.06689900\n",
            "Iteration 390, loss = 0.06656159\n",
            "Iteration 391, loss = 0.06639120\n",
            "Iteration 392, loss = 0.06615176\n",
            "Iteration 393, loss = 0.06597546\n",
            "Iteration 394, loss = 0.06573762\n",
            "Iteration 395, loss = 0.06546945\n",
            "Iteration 396, loss = 0.06518741\n",
            "Iteration 397, loss = 0.06498789\n",
            "Iteration 398, loss = 0.06473105\n",
            "Iteration 399, loss = 0.06454791\n",
            "Iteration 400, loss = 0.06428440\n",
            "Iteration 401, loss = 0.06417322\n",
            "Iteration 402, loss = 0.06385051\n",
            "Iteration 403, loss = 0.06368178\n",
            "Iteration 404, loss = 0.06332530\n",
            "Iteration 405, loss = 0.06310901\n",
            "Iteration 406, loss = 0.06295857\n",
            "Iteration 407, loss = 0.06267682\n",
            "Iteration 408, loss = 0.06248366\n",
            "Iteration 409, loss = 0.06223454\n",
            "Iteration 410, loss = 0.06207148\n",
            "Iteration 411, loss = 0.06192917\n",
            "Iteration 412, loss = 0.06173024\n",
            "Iteration 413, loss = 0.06146757\n",
            "Iteration 414, loss = 0.06129548\n",
            "Iteration 415, loss = 0.06109561\n",
            "Iteration 416, loss = 0.06093240\n",
            "Iteration 417, loss = 0.06070124\n",
            "Iteration 418, loss = 0.06049255\n",
            "Iteration 419, loss = 0.06033716\n",
            "Iteration 420, loss = 0.06002358\n",
            "Iteration 421, loss = 0.05983885\n",
            "Iteration 422, loss = 0.05979394\n",
            "Iteration 423, loss = 0.05952337\n",
            "Iteration 424, loss = 0.05926646\n",
            "Iteration 425, loss = 0.05911422\n",
            "Iteration 426, loss = 0.05885290\n",
            "Iteration 427, loss = 0.05864614\n",
            "Iteration 428, loss = 0.05841495\n",
            "Iteration 429, loss = 0.05860240\n",
            "Iteration 430, loss = 0.05826786\n",
            "Iteration 431, loss = 0.05792223\n",
            "Iteration 432, loss = 0.05773226\n",
            "Iteration 433, loss = 0.05761017\n",
            "Iteration 434, loss = 0.05741085\n",
            "Iteration 435, loss = 0.05735277\n",
            "Iteration 436, loss = 0.05708082\n",
            "Iteration 437, loss = 0.05683157\n",
            "Iteration 438, loss = 0.05669834\n",
            "Iteration 439, loss = 0.05658165\n",
            "Iteration 440, loss = 0.05629087\n",
            "Iteration 441, loss = 0.05621639\n",
            "Iteration 442, loss = 0.05608821\n",
            "Iteration 443, loss = 0.05584976\n",
            "Iteration 444, loss = 0.05563547\n",
            "Iteration 445, loss = 0.05543032\n",
            "Iteration 446, loss = 0.05527146\n",
            "Iteration 447, loss = 0.05512526\n",
            "Iteration 448, loss = 0.05563934\n",
            "Iteration 449, loss = 0.05500916\n",
            "Iteration 450, loss = 0.05457982\n",
            "Iteration 451, loss = 0.05455608\n",
            "Iteration 452, loss = 0.05472303\n",
            "Iteration 453, loss = 0.05449165\n",
            "Iteration 454, loss = 0.05416760\n",
            "Iteration 455, loss = 0.05397300\n",
            "Iteration 456, loss = 0.05374850\n",
            "Iteration 457, loss = 0.05357764\n",
            "Iteration 458, loss = 0.05340848\n",
            "Iteration 459, loss = 0.05332060\n",
            "Iteration 460, loss = 0.05302398\n",
            "Iteration 461, loss = 0.05295498\n",
            "Iteration 462, loss = 0.05274085\n",
            "Iteration 463, loss = 0.05259153\n",
            "Iteration 464, loss = 0.05248148\n",
            "Iteration 465, loss = 0.05226893\n",
            "Iteration 466, loss = 0.05215568\n",
            "Iteration 467, loss = 0.05196298\n",
            "Iteration 468, loss = 0.05181571\n",
            "Iteration 469, loss = 0.05172615\n",
            "Iteration 470, loss = 0.05156928\n",
            "Iteration 471, loss = 0.05146597\n",
            "Iteration 472, loss = 0.05127400\n",
            "Iteration 473, loss = 0.05111342\n",
            "Iteration 474, loss = 0.05090400\n",
            "Iteration 475, loss = 0.05093884\n",
            "Iteration 476, loss = 0.05066420\n",
            "Iteration 477, loss = 0.05051788\n",
            "Iteration 478, loss = 0.05042470\n",
            "Iteration 479, loss = 0.05036941\n",
            "Iteration 480, loss = 0.05024456\n",
            "Iteration 481, loss = 0.04997469\n",
            "Iteration 482, loss = 0.04976593\n",
            "Iteration 483, loss = 0.04971757\n",
            "Iteration 484, loss = 0.04956424\n",
            "Iteration 485, loss = 0.04950452\n",
            "Iteration 486, loss = 0.04934455\n",
            "Iteration 487, loss = 0.04925445\n",
            "Iteration 488, loss = 0.04899771\n",
            "Iteration 489, loss = 0.04894826\n",
            "Iteration 490, loss = 0.04874072\n",
            "Iteration 491, loss = 0.04863862\n",
            "Iteration 492, loss = 0.04845914\n",
            "Iteration 493, loss = 0.04837171\n",
            "Iteration 494, loss = 0.04818588\n",
            "Iteration 495, loss = 0.04832059\n",
            "Iteration 496, loss = 0.04796250\n",
            "Iteration 497, loss = 0.04784643\n",
            "Iteration 498, loss = 0.04766845\n",
            "Iteration 499, loss = 0.04753833\n",
            "Iteration 500, loss = 0.04743356\n",
            "Iteration 501, loss = 0.04742801\n",
            "Iteration 502, loss = 0.04721686\n",
            "Iteration 503, loss = 0.04707723\n",
            "Iteration 504, loss = 0.04692737\n",
            "Iteration 505, loss = 0.04693094\n",
            "Iteration 506, loss = 0.04686770\n",
            "Iteration 507, loss = 0.04658457\n",
            "Iteration 508, loss = 0.04654691\n",
            "Iteration 509, loss = 0.04636441\n",
            "Iteration 510, loss = 0.04647011\n",
            "Iteration 511, loss = 0.04614675\n",
            "Iteration 512, loss = 0.04601417\n",
            "Iteration 513, loss = 0.04603857\n",
            "Iteration 514, loss = 0.04608167\n",
            "Iteration 515, loss = 0.04594851\n",
            "Iteration 516, loss = 0.04558622\n",
            "Iteration 517, loss = 0.04545345\n",
            "Iteration 518, loss = 0.04543120\n",
            "Iteration 519, loss = 0.04539478\n",
            "Iteration 520, loss = 0.04515533\n",
            "Iteration 521, loss = 0.04503217\n",
            "Iteration 522, loss = 0.04490081\n",
            "Iteration 523, loss = 0.04489753\n",
            "Iteration 524, loss = 0.04474352\n",
            "Iteration 525, loss = 0.04461740\n",
            "Iteration 526, loss = 0.04458640\n",
            "Iteration 527, loss = 0.04439164\n",
            "Iteration 528, loss = 0.04422156\n",
            "Iteration 529, loss = 0.04420067\n",
            "Iteration 530, loss = 0.04416882\n",
            "Iteration 531, loss = 0.04394986\n",
            "Iteration 532, loss = 0.04390323\n",
            "Iteration 533, loss = 0.04368082\n",
            "Iteration 534, loss = 0.04360494\n",
            "Iteration 535, loss = 0.04359508\n",
            "Iteration 536, loss = 0.04348667\n",
            "Iteration 537, loss = 0.04341465\n",
            "Iteration 538, loss = 0.04331133\n",
            "Iteration 539, loss = 0.04317539\n",
            "Iteration 540, loss = 0.04299571\n",
            "Iteration 541, loss = 0.04301795\n",
            "Iteration 542, loss = 0.04289030\n",
            "Iteration 543, loss = 0.04271807\n",
            "Iteration 544, loss = 0.04257452\n",
            "Iteration 545, loss = 0.04249532\n",
            "Iteration 546, loss = 0.04253458\n",
            "Iteration 547, loss = 0.04247649\n",
            "Iteration 548, loss = 0.04231926\n",
            "Iteration 549, loss = 0.04218728\n",
            "Iteration 550, loss = 0.04215769\n",
            "Iteration 551, loss = 0.04224128\n",
            "Iteration 552, loss = 0.04206251\n",
            "Iteration 553, loss = 0.04186913\n",
            "Iteration 554, loss = 0.04168499\n",
            "Iteration 555, loss = 0.04188806\n",
            "Iteration 556, loss = 0.04157575\n",
            "Iteration 557, loss = 0.04152849\n",
            "Iteration 558, loss = 0.04143374\n",
            "Iteration 559, loss = 0.04146754\n",
            "Iteration 560, loss = 0.04104607\n",
            "Iteration 561, loss = 0.04097165\n",
            "Iteration 562, loss = 0.04128352\n",
            "Iteration 563, loss = 0.04095792\n",
            "Iteration 564, loss = 0.04072301\n",
            "Iteration 565, loss = 0.04072794\n",
            "Iteration 566, loss = 0.04077082\n",
            "Iteration 567, loss = 0.04059613\n",
            "Iteration 568, loss = 0.04053353\n",
            "Iteration 569, loss = 0.04034037\n",
            "Iteration 570, loss = 0.04032635\n",
            "Iteration 571, loss = 0.04017789\n",
            "Iteration 572, loss = 0.04009167\n",
            "Iteration 573, loss = 0.04017258\n",
            "Iteration 574, loss = 0.03995904\n",
            "Iteration 575, loss = 0.03982401\n",
            "Iteration 576, loss = 0.03976322\n",
            "Iteration 577, loss = 0.03963375\n",
            "Iteration 578, loss = 0.03972518\n",
            "Iteration 579, loss = 0.03978012\n",
            "Iteration 580, loss = 0.03950111\n",
            "Iteration 581, loss = 0.03934156\n",
            "Iteration 582, loss = 0.03924744\n",
            "Iteration 583, loss = 0.03917996\n",
            "Iteration 584, loss = 0.03930870\n",
            "Iteration 585, loss = 0.03911689\n",
            "Iteration 586, loss = 0.03909149\n",
            "Iteration 587, loss = 0.03894577\n",
            "Iteration 588, loss = 0.03883847\n",
            "Iteration 589, loss = 0.03870546\n",
            "Iteration 590, loss = 0.03868065\n",
            "Iteration 591, loss = 0.03858408\n",
            "Iteration 592, loss = 0.03846242\n",
            "Iteration 593, loss = 0.03840661\n",
            "Iteration 594, loss = 0.03838709\n",
            "Iteration 595, loss = 0.03829237\n",
            "Iteration 596, loss = 0.03820042\n",
            "Iteration 597, loss = 0.03807947\n",
            "Iteration 598, loss = 0.03803390\n",
            "Iteration 599, loss = 0.03803520\n",
            "Iteration 600, loss = 0.03786055\n",
            "Iteration 601, loss = 0.03784716\n",
            "Iteration 602, loss = 0.03775670\n",
            "Iteration 603, loss = 0.03762369\n",
            "Iteration 604, loss = 0.03759937\n",
            "Iteration 605, loss = 0.03755630\n",
            "Iteration 606, loss = 0.03739971\n",
            "Iteration 607, loss = 0.03742357\n",
            "Iteration 608, loss = 0.03730084\n",
            "Iteration 609, loss = 0.03724067\n",
            "Iteration 610, loss = 0.03717209\n",
            "Iteration 611, loss = 0.03725433\n",
            "Iteration 612, loss = 0.03716832\n",
            "Iteration 613, loss = 0.03698712\n",
            "Iteration 614, loss = 0.03691420\n",
            "Iteration 615, loss = 0.03704155\n",
            "Iteration 616, loss = 0.03683306\n",
            "Iteration 617, loss = 0.03670283\n",
            "Iteration 618, loss = 0.03673302\n",
            "Iteration 619, loss = 0.03657296\n",
            "Iteration 620, loss = 0.03648679\n",
            "Iteration 621, loss = 0.03651674\n",
            "Iteration 622, loss = 0.03635480\n",
            "Iteration 623, loss = 0.03633022\n",
            "Iteration 624, loss = 0.03619335\n",
            "Iteration 625, loss = 0.03627947\n",
            "Iteration 626, loss = 0.03622980\n",
            "Iteration 627, loss = 0.03616216\n",
            "Iteration 628, loss = 0.03610855\n",
            "Iteration 629, loss = 0.03597235\n",
            "Iteration 630, loss = 0.03590868\n",
            "Iteration 631, loss = 0.03587729\n",
            "Iteration 632, loss = 0.03571353\n",
            "Iteration 633, loss = 0.03569957\n",
            "Iteration 634, loss = 0.03565448\n",
            "Iteration 635, loss = 0.03563340\n",
            "Iteration 636, loss = 0.03559126\n",
            "Iteration 637, loss = 0.03548084\n",
            "Iteration 638, loss = 0.03536386\n",
            "Iteration 639, loss = 0.03539254\n",
            "Iteration 640, loss = 0.03535962\n",
            "Iteration 641, loss = 0.03531415\n",
            "Iteration 642, loss = 0.03535548\n",
            "Iteration 643, loss = 0.03510198\n",
            "Iteration 644, loss = 0.03502839\n",
            "Iteration 645, loss = 0.03514152\n",
            "Iteration 646, loss = 0.03488336\n",
            "Iteration 647, loss = 0.03494917\n",
            "Iteration 648, loss = 0.03475623\n",
            "Iteration 649, loss = 0.03474849\n",
            "Iteration 650, loss = 0.03461562\n",
            "Iteration 651, loss = 0.03476558\n",
            "Iteration 652, loss = 0.03472211\n",
            "Iteration 653, loss = 0.03461710\n",
            "Iteration 654, loss = 0.03454207\n",
            "Iteration 655, loss = 0.03438079\n",
            "Iteration 656, loss = 0.03436053\n",
            "Iteration 657, loss = 0.03439753\n",
            "Iteration 658, loss = 0.03437209\n",
            "Iteration 659, loss = 0.03440051\n",
            "Iteration 660, loss = 0.03435461\n",
            "Iteration 661, loss = 0.03420831\n",
            "Iteration 662, loss = 0.03407159\n",
            "Iteration 663, loss = 0.03394413\n",
            "Iteration 664, loss = 0.03404646\n",
            "Iteration 665, loss = 0.03396786\n",
            "Iteration 666, loss = 0.03378179\n",
            "Iteration 667, loss = 0.03375329\n",
            "Iteration 668, loss = 0.03380535\n",
            "Iteration 669, loss = 0.03374247\n",
            "Iteration 670, loss = 0.03372313\n",
            "Iteration 671, loss = 0.03353068\n",
            "Iteration 672, loss = 0.03352130\n",
            "Iteration 673, loss = 0.03350388\n",
            "Iteration 674, loss = 0.03342183\n",
            "Iteration 675, loss = 0.03347208\n",
            "Iteration 676, loss = 0.03331450\n",
            "Iteration 677, loss = 0.03340085\n",
            "Iteration 678, loss = 0.03324054\n",
            "Iteration 679, loss = 0.03323198\n",
            "Iteration 680, loss = 0.03311016\n",
            "Iteration 681, loss = 0.03307875\n",
            "Iteration 682, loss = 0.03297551\n",
            "Iteration 683, loss = 0.03301961\n",
            "Iteration 684, loss = 0.03294634\n",
            "Iteration 685, loss = 0.03286821\n",
            "Iteration 686, loss = 0.03284677\n",
            "Iteration 687, loss = 0.03289839\n",
            "Iteration 688, loss = 0.03281338\n",
            "Iteration 689, loss = 0.03268257\n",
            "Iteration 690, loss = 0.03256871\n",
            "Iteration 691, loss = 0.03258865\n",
            "Iteration 692, loss = 0.03262548\n",
            "Iteration 693, loss = 0.03249845\n",
            "Iteration 694, loss = 0.03239294\n",
            "Iteration 695, loss = 0.03240160\n",
            "Iteration 696, loss = 0.03243187\n",
            "Iteration 697, loss = 0.03236651\n",
            "Iteration 698, loss = 0.03245195\n",
            "Iteration 699, loss = 0.03231214\n",
            "Iteration 700, loss = 0.03218062\n",
            "Iteration 701, loss = 0.03225895\n",
            "Iteration 702, loss = 0.03228342\n",
            "Iteration 703, loss = 0.03198562\n",
            "Iteration 704, loss = 0.03193086\n",
            "Iteration 705, loss = 0.03190850\n",
            "Iteration 706, loss = 0.03185356\n",
            "Iteration 707, loss = 0.03187202\n",
            "Iteration 708, loss = 0.03184328\n",
            "Iteration 709, loss = 0.03175804\n",
            "Iteration 710, loss = 0.03178325\n",
            "Iteration 711, loss = 0.03162135\n",
            "Iteration 712, loss = 0.03205935\n",
            "Iteration 713, loss = 0.03175379\n",
            "Iteration 714, loss = 0.03166603\n",
            "Iteration 715, loss = 0.03144618\n",
            "Iteration 716, loss = 0.03164164\n",
            "Iteration 717, loss = 0.03149420\n",
            "Iteration 718, loss = 0.03146950\n",
            "Iteration 719, loss = 0.03126645\n",
            "Iteration 720, loss = 0.03132521\n",
            "Iteration 721, loss = 0.03132383\n",
            "Iteration 722, loss = 0.03131069\n",
            "Iteration 723, loss = 0.03131116\n",
            "Iteration 724, loss = 0.03112501\n",
            "Iteration 725, loss = 0.03118248\n",
            "Iteration 726, loss = 0.03110912\n",
            "Iteration 727, loss = 0.03118701\n",
            "Iteration 728, loss = 0.03095408\n",
            "Iteration 729, loss = 0.03102812\n",
            "Iteration 730, loss = 0.03085025\n",
            "Iteration 731, loss = 0.03086313\n",
            "Iteration 732, loss = 0.03078438\n",
            "Iteration 733, loss = 0.03082716\n",
            "Iteration 734, loss = 0.03076596\n",
            "Iteration 735, loss = 0.03082133\n",
            "Iteration 736, loss = 0.03084326\n",
            "Iteration 737, loss = 0.03067826\n",
            "Iteration 738, loss = 0.03057833\n",
            "Iteration 739, loss = 0.03051371\n",
            "Iteration 740, loss = 0.03060394\n",
            "Iteration 741, loss = 0.03058421\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Accuracy Score: 0.7068965517241379\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78        40\n",
            "           1       0.52      0.61      0.56        18\n",
            "\n",
            "    accuracy                           0.71        58\n",
            "   macro avg       0.67      0.68      0.67        58\n",
            "weighted avg       0.72      0.71      0.71        58\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2"
      ],
      "metadata": {
        "id": "qOTMYFQFbwFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "df = pd.read_csv('/content/breast-cancer.csv')\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_treino_encoded = le.fit_transform(y_treino)\n",
        "\n",
        "y_teste_encoded = le.transform(y_teste)\n",
        "\n",
        "categorical_cols = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\n",
        "\n",
        "X_combined = pd.concat([X_treino, X_teste], axis=0)\n",
        "X_combined_encoded = pd.get_dummies(X_combined, columns=categorical_cols)\n",
        "\n",
        "X_treino_encoded = X_combined_encoded.iloc[:len(X_treino)]\n",
        "X_teste_encoded = X_combined_encoded.iloc[len(X_treino):]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_treino_normalized = scaler.fit_transform(X_treino_encoded)\n",
        "X_teste_normalized = scaler.transform(X_teste_encoded)\n",
        "\n",
        "smote = SMOTE()\n",
        "X_treino_balanced, y_treino_balanced = smote.fit_resample(X_treino_normalized, y_treino_encoded)\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "}\n",
        "\n",
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "grid_search = GridSearchCV(modelo, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores Hiperparâmetros:\", best_params)\n",
        "\n",
        "modelo = MLPClassifier(max_iter=1000, verbose=True, **best_params)\n",
        "modelo.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "previsoes = modelo.predict(X_teste_normalized)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(y_teste_encoded, previsoes))\n",
        "\n",
        "confusion_matrix_plot = ConfusionMatrix(modelo)\n",
        "confusion_matrix_plot.fit(X_treino_balanced, y_treino_balanced)\n",
        "confusion_matrix_plot.score(X_teste_normalized, y_teste_encoded)\n",
        "confusion_matrix_plot.show()\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_teste_encoded, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pKB--w79Z6kf",
        "outputId": "0614eb41-c04c-4e8b-a9f3-15130c74c56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.69387780\n",
            "Iteration 2, loss = 0.67714516\n",
            "Iteration 3, loss = 0.66241069\n",
            "Iteration 4, loss = 0.64978166\n",
            "Iteration 5, loss = 0.63708120\n",
            "Iteration 6, loss = 0.62640047\n",
            "Iteration 7, loss = 0.61502998\n",
            "Iteration 8, loss = 0.60419668\n",
            "Iteration 9, loss = 0.59298161\n",
            "Iteration 10, loss = 0.58235378\n",
            "Iteration 11, loss = 0.57275322\n",
            "Iteration 12, loss = 0.56322757\n",
            "Iteration 13, loss = 0.55465721\n",
            "Iteration 14, loss = 0.54531940\n",
            "Iteration 15, loss = 0.53687000\n",
            "Iteration 16, loss = 0.52786933\n",
            "Iteration 17, loss = 0.51979128\n",
            "Iteration 18, loss = 0.51108385\n",
            "Iteration 19, loss = 0.50265781\n",
            "Iteration 20, loss = 0.49445978\n",
            "Iteration 21, loss = 0.48548767\n",
            "Iteration 22, loss = 0.47648432\n",
            "Iteration 23, loss = 0.46843849\n",
            "Iteration 24, loss = 0.45921950\n",
            "Iteration 25, loss = 0.45008435\n",
            "Iteration 26, loss = 0.44044533\n",
            "Iteration 27, loss = 0.43143582\n",
            "Iteration 28, loss = 0.42170170\n",
            "Iteration 29, loss = 0.41232718\n",
            "Iteration 30, loss = 0.40269250\n",
            "Iteration 31, loss = 0.39345554\n",
            "Iteration 32, loss = 0.38375586\n",
            "Iteration 33, loss = 0.37386099\n",
            "Iteration 34, loss = 0.36513881\n",
            "Iteration 35, loss = 0.35457013\n",
            "Iteration 36, loss = 0.34517097\n",
            "Iteration 37, loss = 0.33599985\n",
            "Iteration 38, loss = 0.32754851\n",
            "Iteration 39, loss = 0.31729449\n",
            "Iteration 40, loss = 0.30850422\n",
            "Iteration 41, loss = 0.29909974\n",
            "Iteration 42, loss = 0.29090627\n",
            "Iteration 43, loss = 0.28134006\n",
            "Iteration 44, loss = 0.27358276\n",
            "Iteration 45, loss = 0.26544232\n",
            "Iteration 46, loss = 0.25605648\n",
            "Iteration 47, loss = 0.24861433\n",
            "Iteration 48, loss = 0.24086505\n",
            "Iteration 49, loss = 0.23267388\n",
            "Iteration 50, loss = 0.22489719\n",
            "Iteration 51, loss = 0.21782664\n",
            "Iteration 52, loss = 0.21058272\n",
            "Iteration 53, loss = 0.20361024\n",
            "Iteration 54, loss = 0.19723472\n",
            "Iteration 55, loss = 0.19074460\n",
            "Iteration 56, loss = 0.18404036\n",
            "Iteration 57, loss = 0.17814558\n",
            "Iteration 58, loss = 0.17144183\n",
            "Iteration 59, loss = 0.16718838\n",
            "Iteration 60, loss = 0.16064797\n",
            "Iteration 61, loss = 0.15638117\n",
            "Iteration 62, loss = 0.15046288\n",
            "Iteration 63, loss = 0.14504536\n",
            "Iteration 64, loss = 0.14072152\n",
            "Iteration 65, loss = 0.13481118\n",
            "Iteration 66, loss = 0.13204686\n",
            "Iteration 67, loss = 0.12726860\n",
            "Iteration 68, loss = 0.12219291\n",
            "Iteration 69, loss = 0.11848163\n",
            "Iteration 70, loss = 0.11420256\n",
            "Iteration 71, loss = 0.11090419\n",
            "Iteration 72, loss = 0.10792416\n",
            "Iteration 73, loss = 0.10306615\n",
            "Iteration 74, loss = 0.10022071\n",
            "Iteration 75, loss = 0.09789974\n",
            "Iteration 76, loss = 0.09432264\n",
            "Iteration 77, loss = 0.09223973\n",
            "Iteration 78, loss = 0.08886548\n",
            "Iteration 79, loss = 0.08571247\n",
            "Iteration 80, loss = 0.08281924\n",
            "Iteration 81, loss = 0.08049335\n",
            "Iteration 82, loss = 0.07883519\n",
            "Iteration 83, loss = 0.07563024\n",
            "Iteration 84, loss = 0.07501988\n",
            "Iteration 85, loss = 0.07380676\n",
            "Iteration 86, loss = 0.06945765\n",
            "Iteration 87, loss = 0.06869187\n",
            "Iteration 88, loss = 0.06635602\n",
            "Iteration 89, loss = 0.06456740\n",
            "Iteration 90, loss = 0.06283552\n",
            "Iteration 91, loss = 0.06076272\n",
            "Iteration 92, loss = 0.06106440\n",
            "Iteration 93, loss = 0.05884205\n",
            "Iteration 94, loss = 0.05722492\n",
            "Iteration 95, loss = 0.05812433\n",
            "Iteration 96, loss = 0.05524593\n",
            "Iteration 97, loss = 0.05334929\n",
            "Iteration 98, loss = 0.05234425\n",
            "Iteration 99, loss = 0.05152863\n",
            "Iteration 100, loss = 0.05072761\n",
            "Iteration 101, loss = 0.04968810\n",
            "Iteration 102, loss = 0.04847439\n",
            "Iteration 103, loss = 0.04750147\n",
            "Iteration 104, loss = 0.04642106\n",
            "Iteration 105, loss = 0.04620005\n",
            "Iteration 106, loss = 0.04516736\n",
            "Iteration 107, loss = 0.04404887\n",
            "Iteration 108, loss = 0.04307583\n",
            "Iteration 109, loss = 0.04256478\n",
            "Iteration 110, loss = 0.04164366\n",
            "Iteration 111, loss = 0.04095285\n",
            "Iteration 112, loss = 0.04022621\n",
            "Iteration 113, loss = 0.03998655\n",
            "Iteration 114, loss = 0.03941490\n",
            "Iteration 115, loss = 0.03875423\n",
            "Iteration 116, loss = 0.03822771\n",
            "Iteration 117, loss = 0.03732136\n",
            "Iteration 118, loss = 0.03691376\n",
            "Iteration 119, loss = 0.03646299\n",
            "Iteration 120, loss = 0.03622383\n",
            "Iteration 121, loss = 0.03562368\n",
            "Iteration 122, loss = 0.03514923\n",
            "Iteration 123, loss = 0.03474524\n",
            "Iteration 124, loss = 0.03405807\n",
            "Iteration 125, loss = 0.03374531\n",
            "Iteration 126, loss = 0.03331746\n",
            "Iteration 127, loss = 0.03312587\n",
            "Iteration 128, loss = 0.03310514\n",
            "Iteration 129, loss = 0.03215187\n",
            "Iteration 130, loss = 0.03173706\n",
            "Iteration 131, loss = 0.03191425\n",
            "Iteration 132, loss = 0.03109873\n",
            "Iteration 133, loss = 0.03121651\n",
            "Iteration 134, loss = 0.03187372\n",
            "Iteration 135, loss = 0.03025358\n",
            "Iteration 136, loss = 0.02966043\n",
            "Iteration 137, loss = 0.03033567\n",
            "Iteration 138, loss = 0.03047337\n",
            "Iteration 139, loss = 0.02932272\n",
            "Iteration 140, loss = 0.02890412\n",
            "Iteration 141, loss = 0.02888925\n",
            "Iteration 142, loss = 0.02850841\n",
            "Iteration 143, loss = 0.02888094\n",
            "Iteration 144, loss = 0.02748342\n",
            "Iteration 145, loss = 0.02880629\n",
            "Iteration 146, loss = 0.02854447\n",
            "Iteration 147, loss = 0.02768582\n",
            "Iteration 148, loss = 0.02786147\n",
            "Iteration 149, loss = 0.02702275\n",
            "Iteration 150, loss = 0.02720542\n",
            "Iteration 151, loss = 0.02709904\n",
            "Iteration 152, loss = 0.02631781\n",
            "Iteration 153, loss = 0.02595868\n",
            "Iteration 154, loss = 0.02686385\n",
            "Iteration 155, loss = 0.02625795\n",
            "Iteration 156, loss = 0.02539173\n",
            "Iteration 157, loss = 0.02610185\n",
            "Iteration 158, loss = 0.02550823\n",
            "Iteration 159, loss = 0.02531165\n",
            "Iteration 160, loss = 0.02613227\n",
            "Iteration 161, loss = 0.02593072\n",
            "Iteration 162, loss = 0.02479845\n",
            "Iteration 163, loss = 0.02508633\n",
            "Iteration 164, loss = 0.02571405\n",
            "Iteration 165, loss = 0.02474890\n",
            "Iteration 166, loss = 0.02462531\n",
            "Iteration 167, loss = 0.02428727\n",
            "Iteration 168, loss = 0.02362194\n",
            "Iteration 169, loss = 0.02549915\n",
            "Iteration 170, loss = 0.02483119\n",
            "Iteration 171, loss = 0.02445132\n",
            "Iteration 172, loss = 0.02586500\n",
            "Iteration 173, loss = 0.02435951\n",
            "Iteration 174, loss = 0.02478081\n",
            "Iteration 175, loss = 0.02495063\n",
            "Iteration 176, loss = 0.02290901\n",
            "Iteration 177, loss = 0.02300149\n",
            "Iteration 178, loss = 0.02283224\n",
            "Iteration 179, loss = 0.02364809\n",
            "Iteration 180, loss = 0.02254525\n",
            "Iteration 181, loss = 0.02289186\n",
            "Iteration 182, loss = 0.02261781\n",
            "Iteration 183, loss = 0.02271179\n",
            "Iteration 184, loss = 0.02287456\n",
            "Iteration 185, loss = 0.02257243\n",
            "Iteration 186, loss = 0.02214532\n",
            "Iteration 187, loss = 0.02191464\n",
            "Iteration 188, loss = 0.02242283\n",
            "Iteration 189, loss = 0.02233325\n",
            "Iteration 190, loss = 0.02295387\n",
            "Iteration 191, loss = 0.02231652\n",
            "Iteration 192, loss = 0.02181213\n",
            "Iteration 193, loss = 0.02138432\n",
            "Iteration 194, loss = 0.02195823\n",
            "Iteration 195, loss = 0.02167644\n",
            "Iteration 196, loss = 0.02169612\n",
            "Iteration 197, loss = 0.02125622\n",
            "Iteration 198, loss = 0.02107430\n",
            "Iteration 199, loss = 0.02132688\n",
            "Iteration 200, loss = 0.02163771\n",
            "Iteration 201, loss = 0.02140939\n",
            "Iteration 202, loss = 0.02167262\n",
            "Iteration 203, loss = 0.02147458\n",
            "Iteration 204, loss = 0.02233681\n",
            "Iteration 205, loss = 0.02073237\n",
            "Iteration 206, loss = 0.02100148\n",
            "Iteration 207, loss = 0.02111396\n",
            "Iteration 208, loss = 0.02136457\n",
            "Iteration 209, loss = 0.02188560\n",
            "Iteration 210, loss = 0.02030834\n",
            "Iteration 211, loss = 0.02090901\n",
            "Iteration 212, loss = 0.02215757\n",
            "Iteration 213, loss = 0.02017419\n",
            "Iteration 214, loss = 0.02126621\n",
            "Iteration 215, loss = 0.02291691\n",
            "Iteration 216, loss = 0.02178255\n",
            "Iteration 217, loss = 0.02014673\n",
            "Iteration 218, loss = 0.02050645\n",
            "Iteration 219, loss = 0.02099273\n",
            "Iteration 220, loss = 0.02026622\n",
            "Iteration 221, loss = 0.02043454\n",
            "Iteration 222, loss = 0.02004844\n",
            "Iteration 223, loss = 0.02063628\n",
            "Iteration 224, loss = 0.02027188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores Hiperparâmetros: {'activation': 'relu', 'hidden_layer_sizes': (100, 100)}\n",
            "Iteration 1, loss = 0.69943715\n",
            "Iteration 2, loss = 0.67946017\n",
            "Iteration 3, loss = 0.66419130\n",
            "Iteration 4, loss = 0.64964415\n",
            "Iteration 5, loss = 0.63511996\n",
            "Iteration 6, loss = 0.62273340\n",
            "Iteration 7, loss = 0.61009888\n",
            "Iteration 8, loss = 0.59846333\n",
            "Iteration 9, loss = 0.58738418\n",
            "Iteration 10, loss = 0.57585907\n",
            "Iteration 11, loss = 0.56604884\n",
            "Iteration 12, loss = 0.55548754\n",
            "Iteration 13, loss = 0.54576045\n",
            "Iteration 14, loss = 0.53617900\n",
            "Iteration 15, loss = 0.52752279\n",
            "Iteration 16, loss = 0.51871851\n",
            "Iteration 17, loss = 0.50961529\n",
            "Iteration 18, loss = 0.50051856\n",
            "Iteration 19, loss = 0.49179651\n",
            "Iteration 20, loss = 0.48252969\n",
            "Iteration 21, loss = 0.47278226\n",
            "Iteration 22, loss = 0.46374010\n",
            "Iteration 23, loss = 0.45440860\n",
            "Iteration 24, loss = 0.44467779\n",
            "Iteration 25, loss = 0.43478628\n",
            "Iteration 26, loss = 0.42499362\n",
            "Iteration 27, loss = 0.41568882\n",
            "Iteration 28, loss = 0.40578044\n",
            "Iteration 29, loss = 0.39614514\n",
            "Iteration 30, loss = 0.38636820\n",
            "Iteration 31, loss = 0.37620937\n",
            "Iteration 32, loss = 0.36589370\n",
            "Iteration 33, loss = 0.35756487\n",
            "Iteration 34, loss = 0.34823341\n",
            "Iteration 35, loss = 0.33695354\n",
            "Iteration 36, loss = 0.32838405\n",
            "Iteration 37, loss = 0.32049243\n",
            "Iteration 38, loss = 0.30967805\n",
            "Iteration 39, loss = 0.30027406\n",
            "Iteration 40, loss = 0.29205296\n",
            "Iteration 41, loss = 0.28150804\n",
            "Iteration 42, loss = 0.27391664\n",
            "Iteration 43, loss = 0.26678899\n",
            "Iteration 44, loss = 0.25630463\n",
            "Iteration 45, loss = 0.24787515\n",
            "Iteration 46, loss = 0.24092800\n",
            "Iteration 47, loss = 0.23145508\n",
            "Iteration 48, loss = 0.22410719\n",
            "Iteration 49, loss = 0.21730151\n",
            "Iteration 50, loss = 0.20843678\n",
            "Iteration 51, loss = 0.20177429\n",
            "Iteration 52, loss = 0.19486153\n",
            "Iteration 53, loss = 0.18817311\n",
            "Iteration 54, loss = 0.18095581\n",
            "Iteration 55, loss = 0.17485826\n",
            "Iteration 56, loss = 0.16927510\n",
            "Iteration 57, loss = 0.16258742\n",
            "Iteration 58, loss = 0.15642912\n",
            "Iteration 59, loss = 0.15113671\n",
            "Iteration 60, loss = 0.14558355\n",
            "Iteration 61, loss = 0.14005252\n",
            "Iteration 62, loss = 0.13574593\n",
            "Iteration 63, loss = 0.13038602\n",
            "Iteration 64, loss = 0.12589965\n",
            "Iteration 65, loss = 0.12219044\n",
            "Iteration 66, loss = 0.11723184\n",
            "Iteration 67, loss = 0.11367310\n",
            "Iteration 68, loss = 0.10946354\n",
            "Iteration 69, loss = 0.10581433\n",
            "Iteration 70, loss = 0.10246530\n",
            "Iteration 71, loss = 0.09876616\n",
            "Iteration 72, loss = 0.09611218\n",
            "Iteration 73, loss = 0.09242289\n",
            "Iteration 74, loss = 0.08981461\n",
            "Iteration 75, loss = 0.08656339\n",
            "Iteration 76, loss = 0.08382742\n",
            "Iteration 77, loss = 0.08150350\n",
            "Iteration 78, loss = 0.07865095\n",
            "Iteration 79, loss = 0.07657404\n",
            "Iteration 80, loss = 0.07422069\n",
            "Iteration 81, loss = 0.07233896\n",
            "Iteration 82, loss = 0.07039116\n",
            "Iteration 83, loss = 0.06817700\n",
            "Iteration 84, loss = 0.06583044\n",
            "Iteration 85, loss = 0.06477086\n",
            "Iteration 86, loss = 0.06223298\n",
            "Iteration 87, loss = 0.06064331\n",
            "Iteration 88, loss = 0.05944179\n",
            "Iteration 89, loss = 0.05774494\n",
            "Iteration 90, loss = 0.05632615\n",
            "Iteration 91, loss = 0.05436879\n",
            "Iteration 92, loss = 0.05340473\n",
            "Iteration 93, loss = 0.05211500\n",
            "Iteration 94, loss = 0.05152232\n",
            "Iteration 95, loss = 0.05005502\n",
            "Iteration 96, loss = 0.04830643\n",
            "Iteration 97, loss = 0.04735259\n",
            "Iteration 98, loss = 0.04601177\n",
            "Iteration 99, loss = 0.04493627\n",
            "Iteration 100, loss = 0.04423271\n",
            "Iteration 101, loss = 0.04332274\n",
            "Iteration 102, loss = 0.04326155\n",
            "Iteration 103, loss = 0.04206063\n",
            "Iteration 104, loss = 0.04149948\n",
            "Iteration 105, loss = 0.03982658\n",
            "Iteration 106, loss = 0.03970933\n",
            "Iteration 107, loss = 0.03889288\n",
            "Iteration 108, loss = 0.03785179\n",
            "Iteration 109, loss = 0.03752260\n",
            "Iteration 110, loss = 0.03689049\n",
            "Iteration 111, loss = 0.03600109\n",
            "Iteration 112, loss = 0.03547306\n",
            "Iteration 113, loss = 0.03499114\n",
            "Iteration 114, loss = 0.03448470\n",
            "Iteration 115, loss = 0.03465043\n",
            "Iteration 116, loss = 0.03404493\n",
            "Iteration 117, loss = 0.03320032\n",
            "Iteration 118, loss = 0.03231560\n",
            "Iteration 119, loss = 0.03222843\n",
            "Iteration 120, loss = 0.03200212\n",
            "Iteration 121, loss = 0.03152922\n",
            "Iteration 122, loss = 0.03180928\n",
            "Iteration 123, loss = 0.03061250\n",
            "Iteration 124, loss = 0.02990610\n",
            "Iteration 125, loss = 0.03018009\n",
            "Iteration 126, loss = 0.02980406\n",
            "Iteration 127, loss = 0.02907773\n",
            "Iteration 128, loss = 0.02887600\n",
            "Iteration 129, loss = 0.03050878\n",
            "Iteration 130, loss = 0.02952720\n",
            "Iteration 131, loss = 0.02858205\n",
            "Iteration 132, loss = 0.02886626\n",
            "Iteration 133, loss = 0.02782650\n",
            "Iteration 134, loss = 0.02726035\n",
            "Iteration 135, loss = 0.02884812\n",
            "Iteration 136, loss = 0.02793474\n",
            "Iteration 137, loss = 0.02728147\n",
            "Iteration 138, loss = 0.02693203\n",
            "Iteration 139, loss = 0.02600361\n",
            "Iteration 140, loss = 0.02619734\n",
            "Iteration 141, loss = 0.02621746\n",
            "Iteration 142, loss = 0.02611498\n",
            "Iteration 143, loss = 0.02551351\n",
            "Iteration 144, loss = 0.02542924\n",
            "Iteration 145, loss = 0.02475054\n",
            "Iteration 146, loss = 0.02511237\n",
            "Iteration 147, loss = 0.02521313\n",
            "Iteration 148, loss = 0.02448426\n",
            "Iteration 149, loss = 0.02512926\n",
            "Iteration 150, loss = 0.02478397\n",
            "Iteration 151, loss = 0.02418560\n",
            "Iteration 152, loss = 0.02446107\n",
            "Iteration 153, loss = 0.02347902\n",
            "Iteration 154, loss = 0.02371147\n",
            "Iteration 155, loss = 0.02341620\n",
            "Iteration 156, loss = 0.02329151\n",
            "Iteration 157, loss = 0.02408024\n",
            "Iteration 158, loss = 0.02302100\n",
            "Iteration 159, loss = 0.02273729\n",
            "Iteration 160, loss = 0.02402303\n",
            "Iteration 161, loss = 0.02242222\n",
            "Iteration 162, loss = 0.02275041\n",
            "Iteration 163, loss = 0.02457208\n",
            "Iteration 164, loss = 0.02430993\n",
            "Iteration 165, loss = 0.02298405\n",
            "Iteration 166, loss = 0.02249107\n",
            "Iteration 167, loss = 0.02209934\n",
            "Iteration 168, loss = 0.02245826\n",
            "Iteration 169, loss = 0.02227860\n",
            "Iteration 170, loss = 0.02162775\n",
            "Iteration 171, loss = 0.02235175\n",
            "Iteration 172, loss = 0.02264671\n",
            "Iteration 173, loss = 0.02184633\n",
            "Iteration 174, loss = 0.02163979\n",
            "Iteration 175, loss = 0.02198455\n",
            "Iteration 176, loss = 0.02189400\n",
            "Iteration 177, loss = 0.02107903\n",
            "Iteration 178, loss = 0.02213687\n",
            "Iteration 179, loss = 0.02259804\n",
            "Iteration 180, loss = 0.02130631\n",
            "Iteration 181, loss = 0.02264033\n",
            "Iteration 182, loss = 0.02110894\n",
            "Iteration 183, loss = 0.02087091\n",
            "Iteration 184, loss = 0.02139697\n",
            "Iteration 185, loss = 0.02167692\n",
            "Iteration 186, loss = 0.02154655\n",
            "Iteration 187, loss = 0.02056453\n",
            "Iteration 188, loss = 0.02137335\n",
            "Iteration 189, loss = 0.02069662\n",
            "Iteration 190, loss = 0.02105305\n",
            "Iteration 191, loss = 0.02050014\n",
            "Iteration 192, loss = 0.02046003\n",
            "Iteration 193, loss = 0.02041108\n",
            "Iteration 194, loss = 0.02026486\n",
            "Iteration 195, loss = 0.02048618\n",
            "Iteration 196, loss = 0.02168007\n",
            "Iteration 197, loss = 0.02070785\n",
            "Iteration 198, loss = 0.02060376\n",
            "Iteration 199, loss = 0.01996382\n",
            "Iteration 200, loss = 0.01993224\n",
            "Iteration 201, loss = 0.02009976\n",
            "Iteration 202, loss = 0.01984018\n",
            "Iteration 203, loss = 0.01974888\n",
            "Iteration 204, loss = 0.01942578\n",
            "Iteration 205, loss = 0.02057487\n",
            "Iteration 206, loss = 0.01989809\n",
            "Iteration 207, loss = 0.01966354\n",
            "Iteration 208, loss = 0.02005224\n",
            "Iteration 209, loss = 0.02061376\n",
            "Iteration 210, loss = 0.02031939\n",
            "Iteration 211, loss = 0.01903777\n",
            "Iteration 212, loss = 0.01959198\n",
            "Iteration 213, loss = 0.02181561\n",
            "Iteration 214, loss = 0.01992930\n",
            "Iteration 215, loss = 0.02040554\n",
            "Iteration 216, loss = 0.02105500\n",
            "Iteration 217, loss = 0.01977839\n",
            "Iteration 218, loss = 0.02025253\n",
            "Iteration 219, loss = 0.01976563\n",
            "Iteration 220, loss = 0.01914507\n",
            "Iteration 221, loss = 0.01910171\n",
            "Iteration 222, loss = 0.01982922\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Accuracy Score: 0.6551724137931034\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.75      0.75        40\n",
            "           1       0.44      0.44      0.44        18\n",
            "\n",
            "    accuracy                           0.66        58\n",
            "   macro avg       0.60      0.60      0.60        58\n",
            "weighted avg       0.66      0.66      0.66        58\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3"
      ],
      "metadata": {
        "id": "Nbqqw5PacRWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo, utiliza-se a lista 'learning_rates' para iterar sobre diferentes taxas de aprendizado. O número de épocas necessário para convergência é então armazenado na lista 'epochs_needed'"
      ],
      "metadata": {
        "id": "XGM4DOU4caj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "df = pd.read_csv('/content/breast-cancer.csv')\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_treino_encoded = le.fit_transform(y_treino)\n",
        "\n",
        "y_teste_encoded = le.transform(y_teste)\n",
        "\n",
        "categorical_cols = ['age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\n",
        "\n",
        "X_combined = pd.concat([X_treino, X_teste], axis=0)\n",
        "X_combined_encoded = pd.get_dummies(X_combined, columns=categorical_cols)\n",
        "\n",
        "X_treino_encoded = X_combined_encoded.iloc[:len(X_treino)]\n",
        "X_teste_encoded = X_combined_encoded.iloc[len(X_treino):]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_treino_normalized = scaler.fit_transform(X_treino_encoded)\n",
        "X_teste_normalized = scaler.transform(X_teste_encoded)\n",
        "\n",
        "smote = SMOTE()\n",
        "X_treino_balanced, y_treino_balanced = smote.fit_resample(X_treino_normalized, y_treino_encoded)\n",
        "\n",
        "# Avaliar a relação entre taxa de aprendizado e épocas\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "epochs_needed = []\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    modelo = MLPClassifier(max_iter=1000, learning_rate_init=learning_rate, verbose=True)\n",
        "    modelo.fit(X_treino_balanced, y_treino_balanced)\n",
        "    epochs_needed.append(modelo.n_iter_)\n",
        "\n",
        "for lr, epochs in zip(learning_rates, epochs_needed):\n",
        "    print(f'Taxa de Aprendizado: {lr}, Épocas Necessárias: {epochs}')\n",
        "\n",
        "# Restante do seu código permanece o mesmo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5xdhKT8b0Gf",
        "outputId": "e6bbe91c-a924-4c08-d57d-28723187a46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.74399191\n",
            "Iteration 2, loss = 0.72979254\n",
            "Iteration 3, loss = 0.71832840\n",
            "Iteration 4, loss = 0.70620107\n",
            "Iteration 5, loss = 0.69596566\n",
            "Iteration 6, loss = 0.68708216\n",
            "Iteration 7, loss = 0.67802397\n",
            "Iteration 8, loss = 0.67003556\n",
            "Iteration 9, loss = 0.66296635\n",
            "Iteration 10, loss = 0.65548601\n",
            "Iteration 11, loss = 0.64915093\n",
            "Iteration 12, loss = 0.64288706\n",
            "Iteration 13, loss = 0.63634650\n",
            "Iteration 14, loss = 0.63081074\n",
            "Iteration 15, loss = 0.62548148\n",
            "Iteration 16, loss = 0.61986268\n",
            "Iteration 17, loss = 0.61518010\n",
            "Iteration 18, loss = 0.61052383\n",
            "Iteration 19, loss = 0.60587878\n",
            "Iteration 20, loss = 0.60133429\n",
            "Iteration 21, loss = 0.59711322\n",
            "Iteration 22, loss = 0.59333302\n",
            "Iteration 23, loss = 0.58914041\n",
            "Iteration 24, loss = 0.58523620\n",
            "Iteration 25, loss = 0.58165900\n",
            "Iteration 26, loss = 0.57799876\n",
            "Iteration 27, loss = 0.57469161\n",
            "Iteration 28, loss = 0.57113203\n",
            "Iteration 29, loss = 0.56765859\n",
            "Iteration 30, loss = 0.56426752\n",
            "Iteration 31, loss = 0.56117911\n",
            "Iteration 32, loss = 0.55758461\n",
            "Iteration 33, loss = 0.55438670\n",
            "Iteration 34, loss = 0.55135504\n",
            "Iteration 35, loss = 0.54810643\n",
            "Iteration 36, loss = 0.54499598\n",
            "Iteration 37, loss = 0.54176208\n",
            "Iteration 38, loss = 0.53865781\n",
            "Iteration 39, loss = 0.53573057\n",
            "Iteration 40, loss = 0.53255363\n",
            "Iteration 41, loss = 0.52958547\n",
            "Iteration 42, loss = 0.52664747\n",
            "Iteration 43, loss = 0.52347323\n",
            "Iteration 44, loss = 0.52051989\n",
            "Iteration 45, loss = 0.51755351\n",
            "Iteration 46, loss = 0.51465561\n",
            "Iteration 47, loss = 0.51193143\n",
            "Iteration 48, loss = 0.50869989\n",
            "Iteration 49, loss = 0.50583140\n",
            "Iteration 50, loss = 0.50291692\n",
            "Iteration 51, loss = 0.49997557\n",
            "Iteration 52, loss = 0.49711736\n",
            "Iteration 53, loss = 0.49416307\n",
            "Iteration 54, loss = 0.49113975\n",
            "Iteration 55, loss = 0.48817139\n",
            "Iteration 56, loss = 0.48538384\n",
            "Iteration 57, loss = 0.48244444\n",
            "Iteration 58, loss = 0.47943123\n",
            "Iteration 59, loss = 0.47660366\n",
            "Iteration 60, loss = 0.47367811\n",
            "Iteration 61, loss = 0.47082313\n",
            "Iteration 62, loss = 0.46782391\n",
            "Iteration 63, loss = 0.46493267\n",
            "Iteration 64, loss = 0.46190757\n",
            "Iteration 65, loss = 0.45912431\n",
            "Iteration 66, loss = 0.45605083\n",
            "Iteration 67, loss = 0.45338354\n",
            "Iteration 68, loss = 0.45033727\n",
            "Iteration 69, loss = 0.44748435\n",
            "Iteration 70, loss = 0.44457692\n",
            "Iteration 71, loss = 0.44171049\n",
            "Iteration 72, loss = 0.43894015\n",
            "Iteration 73, loss = 0.43614503\n",
            "Iteration 74, loss = 0.43326249\n",
            "Iteration 75, loss = 0.43037015\n",
            "Iteration 76, loss = 0.42750396\n",
            "Iteration 77, loss = 0.42469938\n",
            "Iteration 78, loss = 0.42193517\n",
            "Iteration 79, loss = 0.41935659\n",
            "Iteration 80, loss = 0.41634812\n",
            "Iteration 81, loss = 0.41344913\n",
            "Iteration 82, loss = 0.41074100\n",
            "Iteration 83, loss = 0.40805397\n",
            "Iteration 84, loss = 0.40515885\n",
            "Iteration 85, loss = 0.40239670\n",
            "Iteration 86, loss = 0.39956798\n",
            "Iteration 87, loss = 0.39702372\n",
            "Iteration 88, loss = 0.39426139\n",
            "Iteration 89, loss = 0.39145413\n",
            "Iteration 90, loss = 0.38918404\n",
            "Iteration 91, loss = 0.38605657\n",
            "Iteration 92, loss = 0.38323670\n",
            "Iteration 93, loss = 0.38060965\n",
            "Iteration 94, loss = 0.37794895\n",
            "Iteration 95, loss = 0.37540526\n",
            "Iteration 96, loss = 0.37275978\n",
            "Iteration 97, loss = 0.37000485\n",
            "Iteration 98, loss = 0.36751404\n",
            "Iteration 99, loss = 0.36459751\n",
            "Iteration 100, loss = 0.36193690\n",
            "Iteration 101, loss = 0.35927835\n",
            "Iteration 102, loss = 0.35676568\n",
            "Iteration 103, loss = 0.35404659\n",
            "Iteration 104, loss = 0.35145811\n",
            "Iteration 105, loss = 0.34886663\n",
            "Iteration 106, loss = 0.34617902\n",
            "Iteration 107, loss = 0.34361454\n",
            "Iteration 108, loss = 0.34107995\n",
            "Iteration 109, loss = 0.33852251\n",
            "Iteration 110, loss = 0.33612685\n",
            "Iteration 111, loss = 0.33343618\n",
            "Iteration 112, loss = 0.33119256\n",
            "Iteration 113, loss = 0.32849821\n",
            "Iteration 114, loss = 0.32592674\n",
            "Iteration 115, loss = 0.32361638\n",
            "Iteration 116, loss = 0.32152467\n",
            "Iteration 117, loss = 0.31906998\n",
            "Iteration 118, loss = 0.31648576\n",
            "Iteration 119, loss = 0.31402993\n",
            "Iteration 120, loss = 0.31177922\n",
            "Iteration 121, loss = 0.30937992\n",
            "Iteration 122, loss = 0.30720989\n",
            "Iteration 123, loss = 0.30492796\n",
            "Iteration 124, loss = 0.30243494\n",
            "Iteration 125, loss = 0.30025499\n",
            "Iteration 126, loss = 0.29791037\n",
            "Iteration 127, loss = 0.29563356\n",
            "Iteration 128, loss = 0.29358603\n",
            "Iteration 129, loss = 0.29145144\n",
            "Iteration 130, loss = 0.28918316\n",
            "Iteration 131, loss = 0.28709291\n",
            "Iteration 132, loss = 0.28495873\n",
            "Iteration 133, loss = 0.28283814\n",
            "Iteration 134, loss = 0.28075176\n",
            "Iteration 135, loss = 0.27866977\n",
            "Iteration 136, loss = 0.27661262\n",
            "Iteration 137, loss = 0.27464664\n",
            "Iteration 138, loss = 0.27262238\n",
            "Iteration 139, loss = 0.27079248\n",
            "Iteration 140, loss = 0.26895413\n",
            "Iteration 141, loss = 0.26666558\n",
            "Iteration 142, loss = 0.26490799\n",
            "Iteration 143, loss = 0.26276898\n",
            "Iteration 144, loss = 0.26095356\n",
            "Iteration 145, loss = 0.25907887\n",
            "Iteration 146, loss = 0.25713422\n",
            "Iteration 147, loss = 0.25530005\n",
            "Iteration 148, loss = 0.25351290\n",
            "Iteration 149, loss = 0.25179900\n",
            "Iteration 150, loss = 0.24997939\n",
            "Iteration 151, loss = 0.24832157\n",
            "Iteration 152, loss = 0.24645536\n",
            "Iteration 153, loss = 0.24463774\n",
            "Iteration 154, loss = 0.24302333\n",
            "Iteration 155, loss = 0.24130679\n",
            "Iteration 156, loss = 0.23974316\n",
            "Iteration 157, loss = 0.23794121\n",
            "Iteration 158, loss = 0.23623672\n",
            "Iteration 159, loss = 0.23457187\n",
            "Iteration 160, loss = 0.23326213\n",
            "Iteration 161, loss = 0.23132349\n",
            "Iteration 162, loss = 0.22998930\n",
            "Iteration 163, loss = 0.22814310\n",
            "Iteration 164, loss = 0.22660685\n",
            "Iteration 165, loss = 0.22508000\n",
            "Iteration 166, loss = 0.22358469\n",
            "Iteration 167, loss = 0.22208269\n",
            "Iteration 168, loss = 0.22060007\n",
            "Iteration 169, loss = 0.21881777\n",
            "Iteration 170, loss = 0.21747665\n",
            "Iteration 171, loss = 0.21603270\n",
            "Iteration 172, loss = 0.21458693\n",
            "Iteration 173, loss = 0.21299953\n",
            "Iteration 174, loss = 0.21178308\n",
            "Iteration 175, loss = 0.21016096\n",
            "Iteration 176, loss = 0.20874162\n",
            "Iteration 177, loss = 0.20749261\n",
            "Iteration 178, loss = 0.20595718\n",
            "Iteration 179, loss = 0.20461442\n",
            "Iteration 180, loss = 0.20326860\n",
            "Iteration 181, loss = 0.20203721\n",
            "Iteration 182, loss = 0.20095369\n",
            "Iteration 183, loss = 0.19926772\n",
            "Iteration 184, loss = 0.19807713\n",
            "Iteration 185, loss = 0.19688453\n",
            "Iteration 186, loss = 0.19538946\n",
            "Iteration 187, loss = 0.19429357\n",
            "Iteration 188, loss = 0.19322779\n",
            "Iteration 189, loss = 0.19186212\n",
            "Iteration 190, loss = 0.19107475\n",
            "Iteration 191, loss = 0.18928224\n",
            "Iteration 192, loss = 0.18815849\n",
            "Iteration 193, loss = 0.18713265\n",
            "Iteration 194, loss = 0.18580023\n",
            "Iteration 195, loss = 0.18474635\n",
            "Iteration 196, loss = 0.18345608\n",
            "Iteration 197, loss = 0.18226131\n",
            "Iteration 198, loss = 0.18124612\n",
            "Iteration 199, loss = 0.18030105\n",
            "Iteration 200, loss = 0.17897201\n",
            "Iteration 201, loss = 0.17775324\n",
            "Iteration 202, loss = 0.17664555\n",
            "Iteration 203, loss = 0.17552640\n",
            "Iteration 204, loss = 0.17444540\n",
            "Iteration 205, loss = 0.17356578\n",
            "Iteration 206, loss = 0.17224997\n",
            "Iteration 207, loss = 0.17126182\n",
            "Iteration 208, loss = 0.17021943\n",
            "Iteration 209, loss = 0.16914036\n",
            "Iteration 210, loss = 0.16816970\n",
            "Iteration 211, loss = 0.16707738\n",
            "Iteration 212, loss = 0.16613327\n",
            "Iteration 213, loss = 0.16495367\n",
            "Iteration 214, loss = 0.16412575\n",
            "Iteration 215, loss = 0.16291520\n",
            "Iteration 216, loss = 0.16193844\n",
            "Iteration 217, loss = 0.16103439\n",
            "Iteration 218, loss = 0.16005256\n",
            "Iteration 219, loss = 0.15916940\n",
            "Iteration 220, loss = 0.15813211\n",
            "Iteration 221, loss = 0.15738315\n",
            "Iteration 222, loss = 0.15622364\n",
            "Iteration 223, loss = 0.15528889\n",
            "Iteration 224, loss = 0.15436189\n",
            "Iteration 225, loss = 0.15347065\n",
            "Iteration 226, loss = 0.15252201\n",
            "Iteration 227, loss = 0.15174651\n",
            "Iteration 228, loss = 0.15086970\n",
            "Iteration 229, loss = 0.14980409\n",
            "Iteration 230, loss = 0.14906736\n",
            "Iteration 231, loss = 0.14810536\n",
            "Iteration 232, loss = 0.14716085\n",
            "Iteration 233, loss = 0.14633367\n",
            "Iteration 234, loss = 0.14554952\n",
            "Iteration 235, loss = 0.14463161\n",
            "Iteration 236, loss = 0.14375660\n",
            "Iteration 237, loss = 0.14300019\n",
            "Iteration 238, loss = 0.14217344\n",
            "Iteration 239, loss = 0.14141005\n",
            "Iteration 240, loss = 0.14088428\n",
            "Iteration 241, loss = 0.13983206\n",
            "Iteration 242, loss = 0.13901692\n",
            "Iteration 243, loss = 0.13817244\n",
            "Iteration 244, loss = 0.13743759\n",
            "Iteration 245, loss = 0.13660147\n",
            "Iteration 246, loss = 0.13584712\n",
            "Iteration 247, loss = 0.13507260\n",
            "Iteration 248, loss = 0.13434778\n",
            "Iteration 249, loss = 0.13368855\n",
            "Iteration 250, loss = 0.13288090\n",
            "Iteration 251, loss = 0.13217199\n",
            "Iteration 252, loss = 0.13135876\n",
            "Iteration 253, loss = 0.13073795\n",
            "Iteration 254, loss = 0.12997636\n",
            "Iteration 255, loss = 0.12923279\n",
            "Iteration 256, loss = 0.12854449\n",
            "Iteration 257, loss = 0.12778081\n",
            "Iteration 258, loss = 0.12714010\n",
            "Iteration 259, loss = 0.12648831\n",
            "Iteration 260, loss = 0.12573075\n",
            "Iteration 261, loss = 0.12511368\n",
            "Iteration 262, loss = 0.12439422\n",
            "Iteration 263, loss = 0.12366990\n",
            "Iteration 264, loss = 0.12302227\n",
            "Iteration 265, loss = 0.12253341\n",
            "Iteration 266, loss = 0.12182287\n",
            "Iteration 267, loss = 0.12113581\n",
            "Iteration 268, loss = 0.12082357\n",
            "Iteration 269, loss = 0.12024206\n",
            "Iteration 270, loss = 0.11934694\n",
            "Iteration 271, loss = 0.11863487\n",
            "Iteration 272, loss = 0.11814276\n",
            "Iteration 273, loss = 0.11756834\n",
            "Iteration 274, loss = 0.11670055\n",
            "Iteration 275, loss = 0.11620132\n",
            "Iteration 276, loss = 0.11559657\n",
            "Iteration 277, loss = 0.11491143\n",
            "Iteration 278, loss = 0.11428689\n",
            "Iteration 279, loss = 0.11373626\n",
            "Iteration 280, loss = 0.11317008\n",
            "Iteration 281, loss = 0.11267827\n",
            "Iteration 282, loss = 0.11195525\n",
            "Iteration 283, loss = 0.11138664\n",
            "Iteration 284, loss = 0.11085991\n",
            "Iteration 285, loss = 0.11017614\n",
            "Iteration 286, loss = 0.10971116\n",
            "Iteration 287, loss = 0.10914362\n",
            "Iteration 288, loss = 0.10844432\n",
            "Iteration 289, loss = 0.10791985\n",
            "Iteration 290, loss = 0.10750237\n",
            "Iteration 291, loss = 0.10700681\n",
            "Iteration 292, loss = 0.10639123\n",
            "Iteration 293, loss = 0.10579969\n",
            "Iteration 294, loss = 0.10527991\n",
            "Iteration 295, loss = 0.10470832\n",
            "Iteration 296, loss = 0.10428565\n",
            "Iteration 297, loss = 0.10372593\n",
            "Iteration 298, loss = 0.10335499\n",
            "Iteration 299, loss = 0.10273474\n",
            "Iteration 300, loss = 0.10217676\n",
            "Iteration 301, loss = 0.10183530\n",
            "Iteration 302, loss = 0.10128299\n",
            "Iteration 303, loss = 0.10067808\n",
            "Iteration 304, loss = 0.10023642\n",
            "Iteration 305, loss = 0.09980648\n",
            "Iteration 306, loss = 0.09926724\n",
            "Iteration 307, loss = 0.09878394\n",
            "Iteration 308, loss = 0.09832933\n",
            "Iteration 309, loss = 0.09796136\n",
            "Iteration 310, loss = 0.09755907\n",
            "Iteration 311, loss = 0.09699937\n",
            "Iteration 312, loss = 0.09642749\n",
            "Iteration 313, loss = 0.09595732\n",
            "Iteration 314, loss = 0.09556448\n",
            "Iteration 315, loss = 0.09505049\n",
            "Iteration 316, loss = 0.09465906\n",
            "Iteration 317, loss = 0.09424088\n",
            "Iteration 318, loss = 0.09377547\n",
            "Iteration 319, loss = 0.09340806\n",
            "Iteration 320, loss = 0.09288250\n",
            "Iteration 321, loss = 0.09248967\n",
            "Iteration 322, loss = 0.09215343\n",
            "Iteration 323, loss = 0.09183721\n",
            "Iteration 324, loss = 0.09135445\n",
            "Iteration 325, loss = 0.09079501\n",
            "Iteration 326, loss = 0.09054812\n",
            "Iteration 327, loss = 0.09033348\n",
            "Iteration 328, loss = 0.08978286\n",
            "Iteration 329, loss = 0.08925267\n",
            "Iteration 330, loss = 0.08889699\n",
            "Iteration 331, loss = 0.08855114\n",
            "Iteration 332, loss = 0.08807274\n",
            "Iteration 333, loss = 0.08767413\n",
            "Iteration 334, loss = 0.08732368\n",
            "Iteration 335, loss = 0.08692238\n",
            "Iteration 336, loss = 0.08664189\n",
            "Iteration 337, loss = 0.08625316\n",
            "Iteration 338, loss = 0.08597817\n",
            "Iteration 339, loss = 0.08540748\n",
            "Iteration 340, loss = 0.08523117\n",
            "Iteration 341, loss = 0.08465302\n",
            "Iteration 342, loss = 0.08430106\n",
            "Iteration 343, loss = 0.08405122\n",
            "Iteration 344, loss = 0.08360933\n",
            "Iteration 345, loss = 0.08318111\n",
            "Iteration 346, loss = 0.08292439\n",
            "Iteration 347, loss = 0.08268844\n",
            "Iteration 348, loss = 0.08230367\n",
            "Iteration 349, loss = 0.08207169\n",
            "Iteration 350, loss = 0.08148543\n",
            "Iteration 351, loss = 0.08119505\n",
            "Iteration 352, loss = 0.08088298\n",
            "Iteration 353, loss = 0.08047023\n",
            "Iteration 354, loss = 0.08010862\n",
            "Iteration 355, loss = 0.07983434\n",
            "Iteration 356, loss = 0.07948725\n",
            "Iteration 357, loss = 0.07918445\n",
            "Iteration 358, loss = 0.07879190\n",
            "Iteration 359, loss = 0.07846021\n",
            "Iteration 360, loss = 0.07816571\n",
            "Iteration 361, loss = 0.07794305\n",
            "Iteration 362, loss = 0.07752849\n",
            "Iteration 363, loss = 0.07719107\n",
            "Iteration 364, loss = 0.07695977\n",
            "Iteration 365, loss = 0.07659537\n",
            "Iteration 366, loss = 0.07624209\n",
            "Iteration 367, loss = 0.07597789\n",
            "Iteration 368, loss = 0.07566967\n",
            "Iteration 369, loss = 0.07534869\n",
            "Iteration 370, loss = 0.07504117\n",
            "Iteration 371, loss = 0.07478906\n",
            "Iteration 372, loss = 0.07450392\n",
            "Iteration 373, loss = 0.07415142\n",
            "Iteration 374, loss = 0.07394924\n",
            "Iteration 375, loss = 0.07358611\n",
            "Iteration 376, loss = 0.07328985\n",
            "Iteration 377, loss = 0.07299949\n",
            "Iteration 378, loss = 0.07265620\n",
            "Iteration 379, loss = 0.07236760\n",
            "Iteration 380, loss = 0.07237162\n",
            "Iteration 381, loss = 0.07187682\n",
            "Iteration 382, loss = 0.07156789\n",
            "Iteration 383, loss = 0.07144914\n",
            "Iteration 384, loss = 0.07117297\n",
            "Iteration 385, loss = 0.07079759\n",
            "Iteration 386, loss = 0.07046236\n",
            "Iteration 387, loss = 0.07035077\n",
            "Iteration 388, loss = 0.07015170\n",
            "Iteration 389, loss = 0.06980831\n",
            "Iteration 390, loss = 0.06968341\n",
            "Iteration 391, loss = 0.06921787\n",
            "Iteration 392, loss = 0.06897604\n",
            "Iteration 393, loss = 0.06869898\n",
            "Iteration 394, loss = 0.06842553\n",
            "Iteration 395, loss = 0.06819106\n",
            "Iteration 396, loss = 0.06796421\n",
            "Iteration 397, loss = 0.06771413\n",
            "Iteration 398, loss = 0.06739414\n",
            "Iteration 399, loss = 0.06721597\n",
            "Iteration 400, loss = 0.06690341\n",
            "Iteration 401, loss = 0.06673725\n",
            "Iteration 402, loss = 0.06647533\n",
            "Iteration 403, loss = 0.06627477\n",
            "Iteration 404, loss = 0.06598489\n",
            "Iteration 405, loss = 0.06573709\n",
            "Iteration 406, loss = 0.06559075\n",
            "Iteration 407, loss = 0.06530065\n",
            "Iteration 408, loss = 0.06506222\n",
            "Iteration 409, loss = 0.06486489\n",
            "Iteration 410, loss = 0.06461501\n",
            "Iteration 411, loss = 0.06445976\n",
            "Iteration 412, loss = 0.06419709\n",
            "Iteration 413, loss = 0.06394531\n",
            "Iteration 414, loss = 0.06374918\n",
            "Iteration 415, loss = 0.06345978\n",
            "Iteration 416, loss = 0.06330981\n",
            "Iteration 417, loss = 0.06296007\n",
            "Iteration 418, loss = 0.06277455\n",
            "Iteration 419, loss = 0.06259565\n",
            "Iteration 420, loss = 0.06236222\n",
            "Iteration 421, loss = 0.06213467\n",
            "Iteration 422, loss = 0.06193059\n",
            "Iteration 423, loss = 0.06172652\n",
            "Iteration 424, loss = 0.06157141\n",
            "Iteration 425, loss = 0.06129654\n",
            "Iteration 426, loss = 0.06107006\n",
            "Iteration 427, loss = 0.06088796\n",
            "Iteration 428, loss = 0.06073955\n",
            "Iteration 429, loss = 0.06058379\n",
            "Iteration 430, loss = 0.06030749\n",
            "Iteration 431, loss = 0.06013101\n",
            "Iteration 432, loss = 0.05995683\n",
            "Iteration 433, loss = 0.05979248\n",
            "Iteration 434, loss = 0.05946812\n",
            "Iteration 435, loss = 0.05924120\n",
            "Iteration 436, loss = 0.05909902\n",
            "Iteration 437, loss = 0.05894584\n",
            "Iteration 438, loss = 0.05873220\n",
            "Iteration 439, loss = 0.05855690\n",
            "Iteration 440, loss = 0.05829719\n",
            "Iteration 441, loss = 0.05812191\n",
            "Iteration 442, loss = 0.05788015\n",
            "Iteration 443, loss = 0.05791565\n",
            "Iteration 444, loss = 0.05754721\n",
            "Iteration 445, loss = 0.05732304\n",
            "Iteration 446, loss = 0.05713983\n",
            "Iteration 447, loss = 0.05709559\n",
            "Iteration 448, loss = 0.05680019\n",
            "Iteration 449, loss = 0.05662452\n",
            "Iteration 450, loss = 0.05643782\n",
            "Iteration 451, loss = 0.05624373\n",
            "Iteration 452, loss = 0.05609794\n",
            "Iteration 453, loss = 0.05588499\n",
            "Iteration 454, loss = 0.05568839\n",
            "Iteration 455, loss = 0.05558022\n",
            "Iteration 456, loss = 0.05540606\n",
            "Iteration 457, loss = 0.05524955\n",
            "Iteration 458, loss = 0.05501977\n",
            "Iteration 459, loss = 0.05486360\n",
            "Iteration 460, loss = 0.05467233\n",
            "Iteration 461, loss = 0.05455991\n",
            "Iteration 462, loss = 0.05439758\n",
            "Iteration 463, loss = 0.05427711\n",
            "Iteration 464, loss = 0.05409938\n",
            "Iteration 465, loss = 0.05396555\n",
            "Iteration 466, loss = 0.05372144\n",
            "Iteration 467, loss = 0.05356249\n",
            "Iteration 468, loss = 0.05339130\n",
            "Iteration 469, loss = 0.05321630\n",
            "Iteration 470, loss = 0.05303912\n",
            "Iteration 471, loss = 0.05301944\n",
            "Iteration 472, loss = 0.05275758\n",
            "Iteration 473, loss = 0.05261905\n",
            "Iteration 474, loss = 0.05239498\n",
            "Iteration 475, loss = 0.05223846\n",
            "Iteration 476, loss = 0.05233662\n",
            "Iteration 477, loss = 0.05205353\n",
            "Iteration 478, loss = 0.05183829\n",
            "Iteration 479, loss = 0.05169227\n",
            "Iteration 480, loss = 0.05154472\n",
            "Iteration 481, loss = 0.05144089\n",
            "Iteration 482, loss = 0.05130087\n",
            "Iteration 483, loss = 0.05108181\n",
            "Iteration 484, loss = 0.05097723\n",
            "Iteration 485, loss = 0.05082427\n",
            "Iteration 486, loss = 0.05062158\n",
            "Iteration 487, loss = 0.05048154\n",
            "Iteration 488, loss = 0.05035097\n",
            "Iteration 489, loss = 0.05020286\n",
            "Iteration 490, loss = 0.05007941\n",
            "Iteration 491, loss = 0.04993672\n",
            "Iteration 492, loss = 0.04984408\n",
            "Iteration 493, loss = 0.04966888\n",
            "Iteration 494, loss = 0.04955300\n",
            "Iteration 495, loss = 0.04938815\n",
            "Iteration 496, loss = 0.04928354\n",
            "Iteration 497, loss = 0.04913025\n",
            "Iteration 498, loss = 0.04893949\n",
            "Iteration 499, loss = 0.04878279\n",
            "Iteration 500, loss = 0.04872719\n",
            "Iteration 501, loss = 0.04857675\n",
            "Iteration 502, loss = 0.04847530\n",
            "Iteration 503, loss = 0.04831336\n",
            "Iteration 504, loss = 0.04815043\n",
            "Iteration 505, loss = 0.04800500\n",
            "Iteration 506, loss = 0.04794285\n",
            "Iteration 507, loss = 0.04800396\n",
            "Iteration 508, loss = 0.04771217\n",
            "Iteration 509, loss = 0.04753409\n",
            "Iteration 510, loss = 0.04745209\n",
            "Iteration 511, loss = 0.04735570\n",
            "Iteration 512, loss = 0.04721629\n",
            "Iteration 513, loss = 0.04707865\n",
            "Iteration 514, loss = 0.04690568\n",
            "Iteration 515, loss = 0.04681692\n",
            "Iteration 516, loss = 0.04669035\n",
            "Iteration 517, loss = 0.04654463\n",
            "Iteration 518, loss = 0.04650970\n",
            "Iteration 519, loss = 0.04640456\n",
            "Iteration 520, loss = 0.04625959\n",
            "Iteration 521, loss = 0.04616708\n",
            "Iteration 522, loss = 0.04597675\n",
            "Iteration 523, loss = 0.04595764\n",
            "Iteration 524, loss = 0.04579029\n",
            "Iteration 525, loss = 0.04570544\n",
            "Iteration 526, loss = 0.04562920\n",
            "Iteration 527, loss = 0.04551880\n",
            "Iteration 528, loss = 0.04537314\n",
            "Iteration 529, loss = 0.04517341\n",
            "Iteration 530, loss = 0.04503388\n",
            "Iteration 531, loss = 0.04492708\n",
            "Iteration 532, loss = 0.04488843\n",
            "Iteration 533, loss = 0.04467346\n",
            "Iteration 534, loss = 0.04455310\n",
            "Iteration 535, loss = 0.04458985\n",
            "Iteration 536, loss = 0.04443561\n",
            "Iteration 537, loss = 0.04433967\n",
            "Iteration 538, loss = 0.04422859\n",
            "Iteration 539, loss = 0.04420338\n",
            "Iteration 540, loss = 0.04396166\n",
            "Iteration 541, loss = 0.04385618\n",
            "Iteration 542, loss = 0.04380417\n",
            "Iteration 543, loss = 0.04369984\n",
            "Iteration 544, loss = 0.04354315\n",
            "Iteration 545, loss = 0.04351135\n",
            "Iteration 546, loss = 0.04339963\n",
            "Iteration 547, loss = 0.04328607\n",
            "Iteration 548, loss = 0.04313882\n",
            "Iteration 549, loss = 0.04302905\n",
            "Iteration 550, loss = 0.04292956\n",
            "Iteration 551, loss = 0.04295384\n",
            "Iteration 552, loss = 0.04273976\n",
            "Iteration 553, loss = 0.04260557\n",
            "Iteration 554, loss = 0.04258102\n",
            "Iteration 555, loss = 0.04244036\n",
            "Iteration 556, loss = 0.04265929\n",
            "Iteration 557, loss = 0.04219925\n",
            "Iteration 558, loss = 0.04215068\n",
            "Iteration 559, loss = 0.04205417\n",
            "Iteration 560, loss = 0.04194508\n",
            "Iteration 561, loss = 0.04187002\n",
            "Iteration 562, loss = 0.04177055\n",
            "Iteration 563, loss = 0.04176194\n",
            "Iteration 564, loss = 0.04162716\n",
            "Iteration 565, loss = 0.04150891\n",
            "Iteration 566, loss = 0.04136533\n",
            "Iteration 567, loss = 0.04136790\n",
            "Iteration 568, loss = 0.04118809\n",
            "Iteration 569, loss = 0.04114701\n",
            "Iteration 570, loss = 0.04115113\n",
            "Iteration 571, loss = 0.04096187\n",
            "Iteration 572, loss = 0.04092343\n",
            "Iteration 573, loss = 0.04083748\n",
            "Iteration 574, loss = 0.04071824\n",
            "Iteration 575, loss = 0.04068884\n",
            "Iteration 576, loss = 0.04055480\n",
            "Iteration 577, loss = 0.04038541\n",
            "Iteration 578, loss = 0.04035113\n",
            "Iteration 579, loss = 0.04024959\n",
            "Iteration 580, loss = 0.04010356\n",
            "Iteration 581, loss = 0.04007137\n",
            "Iteration 582, loss = 0.03991919\n",
            "Iteration 583, loss = 0.03999219\n",
            "Iteration 584, loss = 0.03987843\n",
            "Iteration 585, loss = 0.03971550\n",
            "Iteration 586, loss = 0.03974287\n",
            "Iteration 587, loss = 0.03972654\n",
            "Iteration 588, loss = 0.03962780\n",
            "Iteration 589, loss = 0.03944259\n",
            "Iteration 590, loss = 0.03931880\n",
            "Iteration 591, loss = 0.03927400\n",
            "Iteration 592, loss = 0.03913878\n",
            "Iteration 593, loss = 0.03912972\n",
            "Iteration 594, loss = 0.03900026\n",
            "Iteration 595, loss = 0.03890156\n",
            "Iteration 596, loss = 0.03884190\n",
            "Iteration 597, loss = 0.03869179\n",
            "Iteration 598, loss = 0.03863297\n",
            "Iteration 599, loss = 0.03853891\n",
            "Iteration 600, loss = 0.03866806\n",
            "Iteration 601, loss = 0.03843146\n",
            "Iteration 602, loss = 0.03832765\n",
            "Iteration 603, loss = 0.03829533\n",
            "Iteration 604, loss = 0.03825592\n",
            "Iteration 605, loss = 0.03822267\n",
            "Iteration 606, loss = 0.03812203\n",
            "Iteration 607, loss = 0.03795535\n",
            "Iteration 608, loss = 0.03790442\n",
            "Iteration 609, loss = 0.03780435\n",
            "Iteration 610, loss = 0.03782564\n",
            "Iteration 611, loss = 0.03773062\n",
            "Iteration 612, loss = 0.03773202\n",
            "Iteration 613, loss = 0.03753959\n",
            "Iteration 614, loss = 0.03744120\n",
            "Iteration 615, loss = 0.03736302\n",
            "Iteration 616, loss = 0.03741508\n",
            "Iteration 617, loss = 0.03729993\n",
            "Iteration 618, loss = 0.03714246\n",
            "Iteration 619, loss = 0.03706073\n",
            "Iteration 620, loss = 0.03700134\n",
            "Iteration 621, loss = 0.03695728\n",
            "Iteration 622, loss = 0.03687369\n",
            "Iteration 623, loss = 0.03682385\n",
            "Iteration 624, loss = 0.03672422\n",
            "Iteration 625, loss = 0.03670292\n",
            "Iteration 626, loss = 0.03659117\n",
            "Iteration 627, loss = 0.03659614\n",
            "Iteration 628, loss = 0.03648524\n",
            "Iteration 629, loss = 0.03639629\n",
            "Iteration 630, loss = 0.03634964\n",
            "Iteration 631, loss = 0.03624171\n",
            "Iteration 632, loss = 0.03618649\n",
            "Iteration 633, loss = 0.03608894\n",
            "Iteration 634, loss = 0.03604945\n",
            "Iteration 635, loss = 0.03597172\n",
            "Iteration 636, loss = 0.03612065\n",
            "Iteration 637, loss = 0.03585322\n",
            "Iteration 638, loss = 0.03582315\n",
            "Iteration 639, loss = 0.03583464\n",
            "Iteration 640, loss = 0.03566434\n",
            "Iteration 641, loss = 0.03559270\n",
            "Iteration 642, loss = 0.03551605\n",
            "Iteration 643, loss = 0.03557471\n",
            "Iteration 644, loss = 0.03542791\n",
            "Iteration 645, loss = 0.03532228\n",
            "Iteration 646, loss = 0.03538669\n",
            "Iteration 647, loss = 0.03525145\n",
            "Iteration 648, loss = 0.03518096\n",
            "Iteration 649, loss = 0.03519468\n",
            "Iteration 650, loss = 0.03512307\n",
            "Iteration 651, loss = 0.03505129\n",
            "Iteration 652, loss = 0.03495015\n",
            "Iteration 653, loss = 0.03485201\n",
            "Iteration 654, loss = 0.03477593\n",
            "Iteration 655, loss = 0.03480477\n",
            "Iteration 656, loss = 0.03475420\n",
            "Iteration 657, loss = 0.03459477\n",
            "Iteration 658, loss = 0.03453617\n",
            "Iteration 659, loss = 0.03445777\n",
            "Iteration 660, loss = 0.03443234\n",
            "Iteration 661, loss = 0.03440677\n",
            "Iteration 662, loss = 0.03440284\n",
            "Iteration 663, loss = 0.03429041\n",
            "Iteration 664, loss = 0.03419907\n",
            "Iteration 665, loss = 0.03420130\n",
            "Iteration 666, loss = 0.03415684\n",
            "Iteration 667, loss = 0.03410054\n",
            "Iteration 668, loss = 0.03409726\n",
            "Iteration 669, loss = 0.03393864\n",
            "Iteration 670, loss = 0.03403654\n",
            "Iteration 671, loss = 0.03384368\n",
            "Iteration 672, loss = 0.03375991\n",
            "Iteration 673, loss = 0.03370526\n",
            "Iteration 674, loss = 0.03369544\n",
            "Iteration 675, loss = 0.03358956\n",
            "Iteration 676, loss = 0.03360984\n",
            "Iteration 677, loss = 0.03347762\n",
            "Iteration 678, loss = 0.03339153\n",
            "Iteration 679, loss = 0.03338874\n",
            "Iteration 680, loss = 0.03342134\n",
            "Iteration 681, loss = 0.03325040\n",
            "Iteration 682, loss = 0.03321646\n",
            "Iteration 683, loss = 0.03328758\n",
            "Iteration 684, loss = 0.03331239\n",
            "Iteration 685, loss = 0.03317990\n",
            "Iteration 686, loss = 0.03302109\n",
            "Iteration 687, loss = 0.03291126\n",
            "Iteration 688, loss = 0.03290908\n",
            "Iteration 689, loss = 0.03290975\n",
            "Iteration 690, loss = 0.03292827\n",
            "Iteration 691, loss = 0.03289426\n",
            "Iteration 692, loss = 0.03279916\n",
            "Iteration 693, loss = 0.03266536\n",
            "Iteration 694, loss = 0.03258034\n",
            "Iteration 695, loss = 0.03267369\n",
            "Iteration 696, loss = 0.03261377\n",
            "Iteration 697, loss = 0.03253752\n",
            "Iteration 698, loss = 0.03245600\n",
            "Iteration 699, loss = 0.03235994\n",
            "Iteration 700, loss = 0.03239511\n",
            "Iteration 701, loss = 0.03230657\n",
            "Iteration 702, loss = 0.03232744\n",
            "Iteration 703, loss = 0.03239911\n",
            "Iteration 704, loss = 0.03235365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68757464\n",
            "Iteration 2, loss = 0.61187157\n",
            "Iteration 3, loss = 0.56745507\n",
            "Iteration 4, loss = 0.54396379\n",
            "Iteration 5, loss = 0.51936772\n",
            "Iteration 6, loss = 0.49695209\n",
            "Iteration 7, loss = 0.47016721\n",
            "Iteration 8, loss = 0.44733066\n",
            "Iteration 9, loss = 0.42409505\n",
            "Iteration 10, loss = 0.40333351\n",
            "Iteration 11, loss = 0.38255471\n",
            "Iteration 12, loss = 0.35958834\n",
            "Iteration 13, loss = 0.33848132\n",
            "Iteration 14, loss = 0.31751499\n",
            "Iteration 15, loss = 0.29829291\n",
            "Iteration 16, loss = 0.27865518\n",
            "Iteration 17, loss = 0.26117050\n",
            "Iteration 18, loss = 0.24322742\n",
            "Iteration 19, loss = 0.22703815\n",
            "Iteration 20, loss = 0.21301050\n",
            "Iteration 21, loss = 0.19829572\n",
            "Iteration 22, loss = 0.18483477\n",
            "Iteration 23, loss = 0.17216279\n",
            "Iteration 24, loss = 0.16152935\n",
            "Iteration 25, loss = 0.15111218\n",
            "Iteration 26, loss = 0.14115305\n",
            "Iteration 27, loss = 0.13271415\n",
            "Iteration 28, loss = 0.12383423\n",
            "Iteration 29, loss = 0.11671626\n",
            "Iteration 30, loss = 0.10965501\n",
            "Iteration 31, loss = 0.10317853\n",
            "Iteration 32, loss = 0.09871727\n",
            "Iteration 33, loss = 0.09313924\n",
            "Iteration 34, loss = 0.08706672\n",
            "Iteration 35, loss = 0.08447462\n",
            "Iteration 36, loss = 0.08226259\n",
            "Iteration 37, loss = 0.07554439\n",
            "Iteration 38, loss = 0.07062289\n",
            "Iteration 39, loss = 0.06972885\n",
            "Iteration 40, loss = 0.06637483\n",
            "Iteration 41, loss = 0.06419489\n",
            "Iteration 42, loss = 0.05990646\n",
            "Iteration 43, loss = 0.05723960\n",
            "Iteration 44, loss = 0.05572597\n",
            "Iteration 45, loss = 0.05508159\n",
            "Iteration 46, loss = 0.05182856\n",
            "Iteration 47, loss = 0.05225548\n",
            "Iteration 48, loss = 0.04800801\n",
            "Iteration 49, loss = 0.04930251\n",
            "Iteration 50, loss = 0.04641670\n",
            "Iteration 51, loss = 0.04748288\n",
            "Iteration 52, loss = 0.04400088\n",
            "Iteration 53, loss = 0.04468729\n",
            "Iteration 54, loss = 0.04170283\n",
            "Iteration 55, loss = 0.04255489\n",
            "Iteration 56, loss = 0.04205562\n",
            "Iteration 57, loss = 0.03876019\n",
            "Iteration 58, loss = 0.03856703\n",
            "Iteration 59, loss = 0.03773032\n",
            "Iteration 60, loss = 0.03602537\n",
            "Iteration 61, loss = 0.03735692\n",
            "Iteration 62, loss = 0.03441396\n",
            "Iteration 63, loss = 0.03477436\n",
            "Iteration 64, loss = 0.03565472\n",
            "Iteration 65, loss = 0.03303914\n",
            "Iteration 66, loss = 0.03592599\n",
            "Iteration 67, loss = 0.03360215\n",
            "Iteration 68, loss = 0.03339419\n",
            "Iteration 69, loss = 0.03360353\n",
            "Iteration 70, loss = 0.03283606\n",
            "Iteration 71, loss = 0.03197794\n",
            "Iteration 72, loss = 0.03197975\n",
            "Iteration 73, loss = 0.03210144\n",
            "Iteration 74, loss = 0.03054945\n",
            "Iteration 75, loss = 0.03171869\n",
            "Iteration 76, loss = 0.02934705\n",
            "Iteration 77, loss = 0.03048376\n",
            "Iteration 78, loss = 0.02949603\n",
            "Iteration 79, loss = 0.02849828\n",
            "Iteration 80, loss = 0.02944117\n",
            "Iteration 81, loss = 0.02947035\n",
            "Iteration 82, loss = 0.02765699\n",
            "Iteration 83, loss = 0.02803750\n",
            "Iteration 84, loss = 0.02764988\n",
            "Iteration 85, loss = 0.02864119\n",
            "Iteration 86, loss = 0.02705104\n",
            "Iteration 87, loss = 0.02675430\n",
            "Iteration 88, loss = 0.02725529\n",
            "Iteration 89, loss = 0.02666556\n",
            "Iteration 90, loss = 0.02722710\n",
            "Iteration 91, loss = 0.02621481\n",
            "Iteration 92, loss = 0.02521104\n",
            "Iteration 93, loss = 0.02583162\n",
            "Iteration 94, loss = 0.02524381\n",
            "Iteration 95, loss = 0.02685375\n",
            "Iteration 96, loss = 0.02591015\n",
            "Iteration 97, loss = 0.02489431\n",
            "Iteration 98, loss = 0.02419166\n",
            "Iteration 99, loss = 0.02450978\n",
            "Iteration 100, loss = 0.02574433\n",
            "Iteration 101, loss = 0.02604427\n",
            "Iteration 102, loss = 0.02494797\n",
            "Iteration 103, loss = 0.02473907\n",
            "Iteration 104, loss = 0.02432560\n",
            "Iteration 105, loss = 0.02512203\n",
            "Iteration 106, loss = 0.02396302\n",
            "Iteration 107, loss = 0.02507179\n",
            "Iteration 108, loss = 0.02494559\n",
            "Iteration 109, loss = 0.02264525\n",
            "Iteration 110, loss = 0.02566532\n",
            "Iteration 111, loss = 0.02556272\n",
            "Iteration 112, loss = 0.02301350\n",
            "Iteration 113, loss = 0.02469620\n",
            "Iteration 114, loss = 0.02377299\n",
            "Iteration 115, loss = 0.02253610\n",
            "Iteration 116, loss = 0.02383136\n",
            "Iteration 117, loss = 0.02311708\n",
            "Iteration 118, loss = 0.02269127\n",
            "Iteration 119, loss = 0.02327049\n",
            "Iteration 120, loss = 0.02394475\n",
            "Iteration 121, loss = 0.02487332\n",
            "Iteration 122, loss = 0.02240670\n",
            "Iteration 123, loss = 0.02418916\n",
            "Iteration 124, loss = 0.02414887\n",
            "Iteration 125, loss = 0.02311341\n",
            "Iteration 126, loss = 0.02246248\n",
            "Iteration 127, loss = 0.02235844\n",
            "Iteration 128, loss = 0.02169611\n",
            "Iteration 129, loss = 0.02313914\n",
            "Iteration 130, loss = 0.02286383\n",
            "Iteration 131, loss = 0.02221223\n",
            "Iteration 132, loss = 0.02205187\n",
            "Iteration 133, loss = 0.02177946\n",
            "Iteration 134, loss = 0.02241434\n",
            "Iteration 135, loss = 0.02132843\n",
            "Iteration 136, loss = 0.02085460\n",
            "Iteration 137, loss = 0.02218313\n",
            "Iteration 138, loss = 0.02180325\n",
            "Iteration 139, loss = 0.02296424\n",
            "Iteration 140, loss = 0.02128298\n",
            "Iteration 141, loss = 0.02128129\n",
            "Iteration 142, loss = 0.02302296\n",
            "Iteration 143, loss = 0.02191006\n",
            "Iteration 144, loss = 0.02157178\n",
            "Iteration 145, loss = 0.02172407\n",
            "Iteration 146, loss = 0.02126484\n",
            "Iteration 147, loss = 0.02058990\n",
            "Iteration 148, loss = 0.02233765\n",
            "Iteration 149, loss = 0.02244444\n",
            "Iteration 150, loss = 0.02256515\n",
            "Iteration 151, loss = 0.01991070\n",
            "Iteration 152, loss = 0.02394603\n",
            "Iteration 153, loss = 0.02444197\n",
            "Iteration 154, loss = 0.02108821\n",
            "Iteration 155, loss = 0.02062504\n",
            "Iteration 156, loss = 0.02182760\n",
            "Iteration 157, loss = 0.02121287\n",
            "Iteration 158, loss = 0.02123797\n",
            "Iteration 159, loss = 0.02149983\n",
            "Iteration 160, loss = 0.02128881\n",
            "Iteration 161, loss = 0.02009270\n",
            "Iteration 162, loss = 0.02057267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.45602562\n",
            "Iteration 2, loss = 0.83683329\n",
            "Iteration 3, loss = 0.71752123\n",
            "Iteration 4, loss = 0.65595822\n",
            "Iteration 5, loss = 0.64761762\n",
            "Iteration 6, loss = 0.62602160\n",
            "Iteration 7, loss = 0.58789471\n",
            "Iteration 8, loss = 0.58554065\n",
            "Iteration 9, loss = 0.55142415\n",
            "Iteration 10, loss = 0.51131708\n",
            "Iteration 11, loss = 0.49124187\n",
            "Iteration 12, loss = 0.46691448\n",
            "Iteration 13, loss = 0.44521613\n",
            "Iteration 14, loss = 0.41711956\n",
            "Iteration 15, loss = 0.39886334\n",
            "Iteration 16, loss = 0.36875545\n",
            "Iteration 17, loss = 0.35183579\n",
            "Iteration 18, loss = 0.33171236\n",
            "Iteration 19, loss = 0.31114297\n",
            "Iteration 20, loss = 0.28960646\n",
            "Iteration 21, loss = 0.27118600\n",
            "Iteration 22, loss = 0.25512749\n",
            "Iteration 23, loss = 0.24501638\n",
            "Iteration 24, loss = 0.22767852\n",
            "Iteration 25, loss = 0.21860402\n",
            "Iteration 26, loss = 0.20154106\n",
            "Iteration 27, loss = 0.18920693\n",
            "Iteration 28, loss = 0.17272184\n",
            "Iteration 29, loss = 0.16257431\n",
            "Iteration 30, loss = 0.15622166\n",
            "Iteration 31, loss = 0.14997618\n",
            "Iteration 32, loss = 0.14395592\n",
            "Iteration 33, loss = 0.13320123\n",
            "Iteration 34, loss = 0.13096485\n",
            "Iteration 35, loss = 0.12092837\n",
            "Iteration 36, loss = 0.11418701\n",
            "Iteration 37, loss = 0.11281765\n",
            "Iteration 38, loss = 0.10711707\n",
            "Iteration 39, loss = 0.10614956\n",
            "Iteration 40, loss = 0.10214847\n",
            "Iteration 41, loss = 0.09273122\n",
            "Iteration 42, loss = 0.09106649\n",
            "Iteration 43, loss = 0.08890763\n",
            "Iteration 44, loss = 0.08449183\n",
            "Iteration 45, loss = 0.08009420\n",
            "Iteration 46, loss = 0.07751272\n",
            "Iteration 47, loss = 0.07533911\n",
            "Iteration 48, loss = 0.07333786\n",
            "Iteration 49, loss = 0.07200407\n",
            "Iteration 50, loss = 0.07083345\n",
            "Iteration 51, loss = 0.06832394\n",
            "Iteration 52, loss = 0.06362073\n",
            "Iteration 53, loss = 0.06273865\n",
            "Iteration 54, loss = 0.05959359\n",
            "Iteration 55, loss = 0.05802878\n",
            "Iteration 56, loss = 0.05523354\n",
            "Iteration 57, loss = 0.05454376\n",
            "Iteration 58, loss = 0.05194144\n",
            "Iteration 59, loss = 0.05434297\n",
            "Iteration 60, loss = 0.05009568\n",
            "Iteration 61, loss = 0.05703770\n",
            "Iteration 62, loss = 0.05085523\n",
            "Iteration 63, loss = 0.05829496\n",
            "Iteration 64, loss = 0.06548157\n",
            "Iteration 65, loss = 0.05107736\n",
            "Iteration 66, loss = 0.05469784\n",
            "Iteration 67, loss = 0.04673013\n",
            "Iteration 68, loss = 0.04645441\n",
            "Iteration 69, loss = 0.04665387\n",
            "Iteration 70, loss = 0.04266133\n",
            "Iteration 71, loss = 0.04466644\n",
            "Iteration 72, loss = 0.04174183\n",
            "Iteration 73, loss = 0.04246898\n",
            "Iteration 74, loss = 0.04011613\n",
            "Iteration 75, loss = 0.04027790\n",
            "Iteration 76, loss = 0.04009024\n",
            "Iteration 77, loss = 0.04079971\n",
            "Iteration 78, loss = 0.03941684\n",
            "Iteration 79, loss = 0.03885627\n",
            "Iteration 80, loss = 0.03640391\n",
            "Iteration 81, loss = 0.03744229\n",
            "Iteration 82, loss = 0.03558535\n",
            "Iteration 83, loss = 0.03102622\n",
            "Iteration 84, loss = 0.03310542\n",
            "Iteration 85, loss = 0.02653841\n",
            "Iteration 86, loss = 0.02797776\n",
            "Iteration 87, loss = 0.02654599\n",
            "Iteration 88, loss = 0.02669474\n",
            "Iteration 89, loss = 0.02524717\n",
            "Iteration 90, loss = 0.02797872\n",
            "Iteration 91, loss = 0.02605702\n",
            "Iteration 92, loss = 0.02779540\n",
            "Iteration 93, loss = 0.02487243\n",
            "Iteration 94, loss = 0.02683780\n",
            "Iteration 95, loss = 0.02317763\n",
            "Iteration 96, loss = 0.02801242\n",
            "Iteration 97, loss = 0.02525322\n",
            "Iteration 98, loss = 0.02166338\n",
            "Iteration 99, loss = 0.02952657\n",
            "Iteration 100, loss = 0.02404214\n",
            "Iteration 101, loss = 0.02735587\n",
            "Iteration 102, loss = 0.02531807\n",
            "Iteration 103, loss = 0.02700636\n",
            "Iteration 104, loss = 0.02456398\n",
            "Iteration 105, loss = 0.02172684\n",
            "Iteration 106, loss = 0.02603377\n",
            "Iteration 107, loss = 0.02622727\n",
            "Iteration 108, loss = 0.02347931\n",
            "Iteration 109, loss = 0.02347924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 7.70580705\n",
            "Iteration 2, loss = 17.62209161\n",
            "Iteration 3, loss = 4.03770633\n",
            "Iteration 4, loss = 3.87076483\n",
            "Iteration 5, loss = 1.91133332\n",
            "Iteration 6, loss = 1.34740709\n",
            "Iteration 7, loss = 0.66828483\n",
            "Iteration 8, loss = 0.65011981\n",
            "Iteration 9, loss = 0.66603520\n",
            "Iteration 10, loss = 0.66719245\n",
            "Iteration 11, loss = 0.66378922\n",
            "Iteration 12, loss = 0.65808016\n",
            "Iteration 13, loss = 0.65673611\n",
            "Iteration 14, loss = 0.65429512\n",
            "Iteration 15, loss = 0.64898141\n",
            "Iteration 16, loss = 0.64491971\n",
            "Iteration 17, loss = 0.63492703\n",
            "Iteration 18, loss = 0.62307282\n",
            "Iteration 19, loss = 0.61551049\n",
            "Iteration 20, loss = 0.60780654\n",
            "Iteration 21, loss = 0.60117862\n",
            "Iteration 22, loss = 0.59143376\n",
            "Iteration 23, loss = 0.59062981\n",
            "Iteration 24, loss = 0.58115668\n",
            "Iteration 25, loss = 0.57604330\n",
            "Iteration 26, loss = 0.57239307\n",
            "Iteration 27, loss = 0.57119393\n",
            "Iteration 28, loss = 0.56812710\n",
            "Iteration 29, loss = 0.56757833\n",
            "Iteration 30, loss = 0.56496562\n",
            "Iteration 31, loss = 0.56415953\n",
            "Iteration 32, loss = 0.56268097\n",
            "Iteration 33, loss = 0.55692073\n",
            "Iteration 34, loss = 0.55775667\n",
            "Iteration 35, loss = 0.55460582\n",
            "Iteration 36, loss = 0.55248835\n",
            "Iteration 37, loss = 0.54860578\n",
            "Iteration 38, loss = 0.54532550\n",
            "Iteration 39, loss = 0.54162830\n",
            "Iteration 40, loss = 0.53942408\n",
            "Iteration 41, loss = 0.54145710\n",
            "Iteration 42, loss = 0.53525045\n",
            "Iteration 43, loss = 0.53862683\n",
            "Iteration 44, loss = 0.53286982\n",
            "Iteration 45, loss = 0.53483440\n",
            "Iteration 46, loss = 0.53425665\n",
            "Iteration 47, loss = 0.52849566\n",
            "Iteration 48, loss = 0.53225747\n",
            "Iteration 49, loss = 0.52859370\n",
            "Iteration 50, loss = 0.52629071\n",
            "Iteration 51, loss = 0.53382923\n",
            "Iteration 52, loss = 0.52791385\n",
            "Iteration 53, loss = 0.51993204\n",
            "Iteration 54, loss = 0.51623426\n",
            "Iteration 55, loss = 0.51409831\n",
            "Iteration 56, loss = 0.51206697\n",
            "Iteration 57, loss = 0.51360836\n",
            "Iteration 58, loss = 0.51072979\n",
            "Iteration 59, loss = 0.51115159\n",
            "Iteration 60, loss = 0.50742782\n",
            "Iteration 61, loss = 0.50829818\n",
            "Iteration 62, loss = 0.50476237\n",
            "Iteration 63, loss = 0.50863707\n",
            "Iteration 64, loss = 0.50918937\n",
            "Iteration 65, loss = 0.51248410\n",
            "Iteration 66, loss = 0.51190572\n",
            "Iteration 67, loss = 0.50726065\n",
            "Iteration 68, loss = 0.50356774\n",
            "Iteration 69, loss = 0.50683116\n",
            "Iteration 70, loss = 0.50440686\n",
            "Iteration 71, loss = 0.50294898\n",
            "Iteration 72, loss = 0.50391399\n",
            "Iteration 73, loss = 0.50487540\n",
            "Iteration 74, loss = 0.50129853\n",
            "Iteration 75, loss = 0.49917231\n",
            "Iteration 76, loss = 0.49737365\n",
            "Iteration 77, loss = 0.49834824\n",
            "Iteration 78, loss = 0.49454928\n",
            "Iteration 79, loss = 0.49245869\n",
            "Iteration 80, loss = 0.49182258\n",
            "Iteration 81, loss = 0.48739690\n",
            "Iteration 82, loss = 0.48764459\n",
            "Iteration 83, loss = 0.48681665\n",
            "Iteration 84, loss = 0.48696018\n",
            "Iteration 85, loss = 0.48796988\n",
            "Iteration 86, loss = 0.48327814\n",
            "Iteration 87, loss = 0.48454157\n",
            "Iteration 88, loss = 0.48326508\n",
            "Iteration 89, loss = 0.48569016\n",
            "Iteration 90, loss = 0.48543905\n",
            "Iteration 91, loss = 0.48620822\n",
            "Iteration 92, loss = 0.48697704\n",
            "Iteration 93, loss = 0.48541662\n",
            "Iteration 94, loss = 0.48669541\n",
            "Iteration 95, loss = 0.48397023\n",
            "Iteration 96, loss = 0.48505862\n",
            "Iteration 97, loss = 0.48306266\n",
            "Iteration 98, loss = 0.48497049\n",
            "Iteration 99, loss = 0.48712372\n",
            "Iteration 100, loss = 0.48662928\n",
            "Iteration 101, loss = 0.48296290\n",
            "Iteration 102, loss = 0.48241310\n",
            "Iteration 103, loss = 0.48020753\n",
            "Iteration 104, loss = 0.47879643\n",
            "Iteration 105, loss = 0.48016165\n",
            "Iteration 106, loss = 0.47797692\n",
            "Iteration 107, loss = 0.47715095\n",
            "Iteration 108, loss = 0.48074763\n",
            "Iteration 109, loss = 0.47914535\n",
            "Iteration 110, loss = 0.47649146\n",
            "Iteration 111, loss = 0.47651761\n",
            "Iteration 112, loss = 0.47604243\n",
            "Iteration 113, loss = 0.47670703\n",
            "Iteration 114, loss = 0.47613929\n",
            "Iteration 115, loss = 0.47821458\n",
            "Iteration 116, loss = 0.47775686\n",
            "Iteration 117, loss = 0.47358561\n",
            "Iteration 118, loss = 0.48278986\n",
            "Iteration 119, loss = 0.47509825\n",
            "Iteration 120, loss = 0.48309555\n",
            "Iteration 121, loss = 0.47870874\n",
            "Iteration 122, loss = 0.48288696\n",
            "Iteration 123, loss = 0.47944073\n",
            "Iteration 124, loss = 0.48455525\n",
            "Iteration 125, loss = 0.47606263\n",
            "Iteration 126, loss = 0.47728263\n",
            "Iteration 127, loss = 0.47423204\n",
            "Iteration 128, loss = 0.47404618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Taxa de Aprendizado: 0.001, Épocas Necessárias: 704\n",
            "Taxa de Aprendizado: 0.01, Épocas Necessárias: 162\n",
            "Taxa de Aprendizado: 0.1, Épocas Necessárias: 109\n",
            "Taxa de Aprendizado: 0.5, Épocas Necessárias: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4"
      ],
      "metadata": {
        "id": "W6ojpdr2fTPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'max_iter': [500, 1000, 1500]\n",
        "}\n",
        "\n",
        "modelo = MLPClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(modelo, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores Hiperparâmetros:\", best_params)\n",
        "\n",
        "# Restante do código permanece o mesmo, usando os melhores hiperparâmetros\n",
        "\n",
        "modelo_otimizado = MLPClassifier(**best_params)\n",
        "modelo_otimizado.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "previsoes_otimizadas = modelo_otimizado.predict(X_teste_normalized)\n",
        "\n",
        "print(\"Accuracy Score (Modelo Otimizado):\", accuracy_score(y_teste_encoded, previsoes_otimizadas))"
      ],
      "metadata": {
        "id": "HTJrDMbJdSAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5\n",
        "Utilizando GridSearch"
      ],
      "metadata": {
        "id": "yw-9yA6FgYTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'max_iter': [500, 1000, 1500]\n",
        "}\n",
        "\n",
        "modelo = MLPClassifier()\n",
        "\n",
        "# Realizar a busca de hiperparâmetros com Grid Search\n",
        "grid_search = GridSearchCV(modelo, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores Hiperparâmetros (Grid Search):\", best_params)\n",
        "\n",
        "# Restante do código permanece o mesmo, usando os melhores hiperparâmetros encontrados\n",
        "\n",
        "modelo_otimizado_grid = MLPClassifier(**best_params)\n",
        "modelo_otimizado_grid.fit(X_treino_balanced, y_treino_balanced)\n",
        "\n",
        "previsoes_otimizadas_grid = modelo_otimizado_grid.predict(X_teste_normalized)\n",
        "\n",
        "print(\"Accuracy Score (Modelo Otimizado - Grid Search):\", accuracy_score(y_teste_encoded, previsoes_otimizadas_grid))"
      ],
      "metadata": {
        "id": "ghqT1QXAgUOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão 06"
      ],
      "metadata": {
        "id": "v9DHW9Exhh8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os artigo aborda extensivamente o desafio da explicação de modelos de caixa preta, fornecendo uma variedade de abordagens para compreender e interpretar as decisões desses modelos complexos. Em primeiro lugar, destaca-se a busca por soluções que utilizam as entranhas de modelos como florestas aleatórias, propondo recomendações para transformar exemplos negativos em positivos, revelando a importância das características do modelo nesse processo.\n",
        "\n",
        "Além disso, os métodos de explicação enfocam a resolução do problema da explicação de resultados de caixas pretas, com ênfase em modelos de aprendizado profundo (DNNs). Abordagens agnósticas são introduzidas, como o Local Interpretable Model-agnostic Explanations (LIME), proporcionando explicações compreensíveis independentemente do tipo de dado ou caixa preta em questão.\n",
        "\n",
        "Na tentativa de entender o funcionamento interno dos modelos de caixa preta, os textos exploram métodos de inspeção que vão desde a análise de sensibilidade até a visualização de dependências parciais. Soluções específicas para DNNs são discutidas, como Máscaras de Saliência e abordagens baseadas em árvores de decisão. Em paralelo, há uma investigação sobre as possíveis armadilhas dos artefatos, garantindo que as explicações geradas sejam representativas de perturbações naturais e evitem resultados inesperados.\n",
        "\n",
        "No contexto do projeto de caixas transparentes, as estratégias incluem a extração de regras, seleção de protótipos e outras abordagens. Métodos como Bayesian Case Model (BCM) e seleção de protótipos transparentes buscam equilibrar interpretabilidade e precisão. Em suma, o artigo oferece uma visão abrangente e aprofundada das diversas técnicas empregadas para desvendar o mistério por trás dos modelos de caixa preta, proporcionando insights valiosos para tornar esses modelos mais compreensíveis e confiáveis."
      ],
      "metadata": {
        "id": "oa7nTfjIh0Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questão 07"
      ],
      "metadata": {
        "id": "EUtvEM2Cpm_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O documentário \"Coded Bias\" é um documentário que explora as implicações éticas e sociais dos algoritmos de aprendizado de máquina e inteligência artificial, com foco particular na questão da discriminação algorítmica e vieses presentes nos sistemas automatizados.\n",
        "\n",
        "O documentário destaca o fenômeno conhecido como \"viés algorítmico\", que ocorre quando algoritmos de IA aprendem a partir de dados históricos que refletem preconceitos e discriminações sociais. Ele aborda questões de justiça social e destaca como certos grupos podem ser prejudicados por decisões automatizadas, como no caso de sistemas de reconhecimento facial que têm demonstrado viés racial.\n",
        "\n",
        "A relação com o termo \"black box\" (caixa preta) e ao artigo do Canvas, está relacionada à opacidade desses algoritmos. Muitos modelos de aprendizado de máquina são complexos e difíceis de interpretar, tornando-se verdadeiras \"caixas pretas\" onde é desafiador entender como chegam a determinadas decisões ou previsões. Isso gera preocupações sobre a falta de transparência e a dificuldade em responsabilizar sistemas automatizados por resultados injustos ou discriminatórios.\n",
        "\n",
        "O documentário \"Coded Bias\" destaca como a falta de compreensão desses modelos pode resultar em impactos negativos na sociedade, especialmente em comunidades historicamente marginalizadas. Ele levanta questões críticas sobre a necessidade de regulamentações, ética e responsabilidade na implementação de algoritmos de IA. Recomendo assistir ao documentário para obter uma compreensão mais aprofundada dessas questões e como elas se relacionam com o conceito de \"black box\"."
      ],
      "metadata": {
        "id": "AR6OnJRBppXd"
      }
    }
  ]
}